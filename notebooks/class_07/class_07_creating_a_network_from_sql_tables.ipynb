{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7: Data Science 2 â€” Querying SQL Tables for Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of today's class\n",
    "1. Get a bunch of reps in using SQL to learn about a dataset.\n",
    "2. Use SQL queries in concert with network science to characterize a network.\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Come in. Sit down. Open Teams.\n",
    "2. Find your notebook in your /Class_07/ folder.\n",
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Dataset\n",
    "\n",
    "Today we're going to focus on getting a lot of reps in in order to learn things about our dataset. It will involve a lot of hands-on coding and less of me talking at you. \n",
    "\n",
    "The dataset we'll be using today comes from SNAP; it's the [Reddit Hyperlinks dataset](https://snap.stanford.edu/data/soc-RedditHyperlinks.html). The paper that arose from this dataset, by Kumar et al., can be found [here](https://cs.stanford.edu/~srijan/pubs/conflict-paper-www18.pdf). \n",
    "\n",
    "Posts in one subreddit can link out to another subreddit for myriad reasons, including hatred, shared interests, and many things in between. I've somewhat simplified the dataset we'll be working with today, but it has some interesting elements nonetheless. For one, each link has a `TIMESTAMP` attribute that indicates when the hyperlink was created. There's also a `LINK_SENTIMENT` attribute that indicates whether the sentiment of the post or title that contained the link was positive (1) or negative (-1). The `link_source` attribute indicates whether the link came from the title or the body of the post. \n",
    "\n",
    "First let's get connected to our database again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "my_creds = []\n",
    "with open('/courses/PHYS7332.202610/shared/student_mysql_credentials.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        my_creds.append(line.strip())\n",
    "\n",
    "hostname=\"mysql-0005\"\n",
    "dbname=\"PHYS7332\"\n",
    "uname=my_creds[0]\n",
    "pwd=my_creds[1]\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "                           .format(host=hostname, db=dbname,\n",
    "                                   user=uname,pw=pwd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "Next, we'll take a look at the dataset. Let's look at the first 10 rows just to get a sense of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qu = 'SELECT * FROM reddit_hyperlinks LIMIT 10;'\n",
    "pd.read_sql(qu, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which linkage pops up the most often in our dataset? Let's find out. \n",
    "\n",
    "We're going to start by grouping our data by the combination of `SOURCE_SUBREDDIT` and `TARGET_SUBREDDIT`. \n",
    "\n",
    "Then we'll count how many rows each grouping contains and note that, along with the names of the source & target subreddits. \n",
    "\n",
    "Finally, we'll sort by the number of total linkages that occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qu = \"\"\"\n",
    "SELECT COUNT(*) as count_links, SOURCE_SUBREDDIT, TARGET_SUBREDDIT \\\n",
    "FROM reddit_hyperlinks \\\n",
    "GROUP BY SOURCE_SUBREDDIT, TARGET_SUBREDDIT ORDER BY count_links;\n",
    "\"\"\"\n",
    "df_link_counts = pd.read_sql(qu, engine)\n",
    "df_link_counts.sort_values('count_links', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distribution\n",
    "You might also be curious about the in- and out- degree distribution of this network. Can you compute the in- and out- degree distributions using the SQL table? In this case, we want to know how many distinct subreddits each other subreddit linked to, not how many **times** a subreddit was linked to. The `DISTINCT` command will be useful here, as will `GROUP BY`; if you want to count distinct values in a column named `MY_COLUMN`, you would use `SELECT COUNT(DISTINCT MY_COLUMN) FROM TABLE;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Turn!\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def degree_distribution(k, number_of_bins=15, log_binning=True, density=True):\n",
    "    \"\"\"\n",
    "    Given a degree sequence, return the y values (probability) and the\n",
    "    x values (support) of a degree distribution that you're going to plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k: a list of nodes' degrees\n",
    "\n",
    "    number_of_bins (int):\n",
    "        length of output vectors\n",
    "    \n",
    "    log_binning (bool):\n",
    "        if you are plotting on a log-log axis, then this is useful\n",
    "    \n",
    "    density (bool):\n",
    "        whether to return counts or probability density (default: True)\n",
    "        Note: probability densities integrate to 1 but do not sum to 1. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    hist, bins (np.ndarray):\n",
    "        probability density if density=True node counts if density=False; binned edges\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    kmax = np.max(k)                    # get the maximum degree\n",
    "    \n",
    "    \n",
    "    # Step 2: Then we'll need to construct bins\n",
    "    if log_binning:\n",
    "        # array of bin edges including rightmost and leftmost\n",
    "        bins = np.logspace(0,np.log10(kmax+1),number_of_bins+1)\n",
    "        bin_edges = []\n",
    "        for ix in range(len(bins) - 1):\n",
    "            bin_edges.append(np.exp((np.log(bins[ix])+np.log(bins[ix + 1]))/2))\n",
    "    else:\n",
    "        bins = np.linspace(0,kmax+1,num=number_of_bins+1)\n",
    "        bin_edges = []\n",
    "        for ix in range(len(bins) - 1):\n",
    "            bin_edges.append((bins[ix] + bins[ix + 1]) / 2)\n",
    "    # Step 3: Then we can compute the histogram using numpy\n",
    "    hist, _ = np.histogram(k,bins,density=density)\n",
    "\n",
    "    return bin_edges, hist\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4),dpi=150)\n",
    "\n",
    "in_degree_bin_edges, in_degree_freqs = degree_distribution(in_degree_list)\n",
    "out_degree_bin_edges, out_degree_freqs = degree_distribution(out_degree_list)\n",
    "ax[0].loglog(in_degree_bin_edges, in_degree_freqs, 'o')\n",
    "ax[0].set_title('in-degree distribution')\n",
    "ax[1].loglog(out_degree_bin_edges, out_degree_freqs, 'o')\n",
    "ax[1].set_title('out-degree distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Dynamics of In-Links\n",
    "Now let's look at how in-links to a specific subreddit show up over time. Do they tend to accumulate steadily over time, or do they arrive in bursts? Let's look at the in-links to /r/askreddit. We'll use the handy `pd.to_datetime` function to convert our string timestamps to datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "qu_inlinks = \"\"\"\n",
    "SELECT TIMESTAMP, SOURCE_SUBREDDIT \\\n",
    "FROM reddit_hyperlinks \\\n",
    "WHERE TARGET_SUBREDDIT='askreddit';\n",
    "\"\"\"\n",
    "df_inlinks = pd.read_sql(qu_inlinks, engine)\n",
    "\n",
    "df_inlinks['ts_rounded_to_month'] = df_inlinks['TIMESTAMP'].apply(pd.to_datetime).dt.to_period('M').dt.to_timestamp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just taken the month floor of the timestamps we computed; this is so we aggregate our in-link counts to every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_per_month = df_inlinks.groupby('ts_rounded_to_month').agg({'TIMESTAMP': 'count'})\n",
    "plt.plot(count_per_month.index, count_per_month['TIMESTAMP'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('in-link frequency')\n",
    "plt.title('/r/askreddit in-link frequency over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at /r/subredditdrama's out-links. Can you make the same plot but for /r/subredditdrama's out-links? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Turn!\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('out-link frequency')\n",
    "plt.title('/r/subredditdrama out-link frequency over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that links can have sentiment attached to them, either positive (1) or negative (-1). Let's look at the number of positive and negative out-links (so plot each one in a different color in the same plot) for /r/subredditdrama over the entire time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Turn!\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('out-link frequency')\n",
    "plt.title('/r/subredditdrama out-link frequency over time by sentiment')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now let's turn to temporal changes in the network. Can you plot the network's in- and out-degree distributions for 2014, 2015, and 2016? Hint: the [`EXTRACT`](https://www.w3schools.com/Sql/func_mysql_extract.asp) function in SQL may come in handy here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(2,3,figsize=(10,4),dpi=150)\n",
    "\n",
    "# Your Turn!\n",
    "for ix, year in enumerate([2014, 2015, 2016]):\n",
    "    in_degree_bin_edges, in_degree_freqs = degree_distribution(in_degree_list)\n",
    "    out_degree_bin_edges, out_degree_freqs = degree_distribution(out_degree_list)\n",
    "    ax[0, ix].loglog(in_degree_bin_edges, in_degree_freqs, 'o')\n",
    "    ax[0, ix].set_title('in-degree {}'.format(str(year)))\n",
    "    ax[1, ix].loglog(out_degree_bin_edges, out_degree_freqs, 'o')\n",
    "    ax[1, ix].set_title('out-degree {}'.format(str(year)))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Clustering\n",
    "Pick a subreddit -- any subreddit (I suggest picking one that has a lot of links to keep things interesting). Can you figure out what its local clustering coefficient is in 2014, 2015, and 2016? Let's assume the graph is unweighted for now, so one hyperlink from subreddit A to subreddit B in 2014 is the same as 10 separate hyperlinks from A to B in the same year. Note that this graph is directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "remove_code_outputs": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Turn\n",
    "def compute_clustering(subreddit_name, year, engine):\n",
    "    \"\"\"\n",
    "    Given the name of a subreddit, a year, and a SQL connection engine,\n",
    "    return the local clustering coefficient of the subreddit over the course of that year,\n",
    "    considering the network as undirected, OR raise appropriate errors.\n",
    "\n",
    "    subreddit_name: string; valid name of a subreddit\n",
    "    year: integer between 2014 and 2017 (inclusive)\n",
    "    engine: sqlalchemy engine object\n",
    "\n",
    "    returns: the local clustering coefficient of the subreddit\n",
    "    \"\"\"\n",
    "    pass\n",
    "compute_clustering('amitheasshole', 2014, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "## Next time...\n",
    "Class 08: Clustering & Community Detection 1 â€” Traditional `class_08_communities1.ipynb`\n",
    "_______\n",
    "\n",
    "## References and further resources:\n",
    "\n",
    "1. Class Webpages\n",
    "    - Jupyter Book: https://network-science-data-and-models.github.io/phys7332_fa25/README.html\n",
    "    - Github: https://github.com/network-science-data-and-models/phys7332_fa25/\n",
    "    - Syllabus and course details: https://brennanklein.com/phys7332-fall25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
