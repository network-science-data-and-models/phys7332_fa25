
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Class 15: Graph Machine Learning 2 &#8212; PHYS 7332: Network Science Data &amp; Models (Fall 2025)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/class_15/class_15_graph_ml_2';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Class 14: Graph Machine Learning 1" href="../class_14/class_14_graph_machine_learning_1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
  
    <p class="title logo__title">PHYS 7332: Network Science Data & Models (Fall 2025)</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    PHYS 7332: Network Science Data & Models – Fall 2025
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../class_00/class_00_github_computing_setup.html">Class 00: Introduction and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_01/class_01_python_refresher.html">Class 01: Python Refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_02/class_02_networkx1.html">Class 02: Introduction to Networkx 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_03/class_03_networkx2.html">Class 03: Introduction to Networkx 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_04/class_04_distributions.html">Class 04: Distributions of Network Properties &amp; Centralities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_05/class_05_scraping1.html">Class 05: Scraping Web Data 1 - BeautifulSoup &amp; HTML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_06/class_06_data_science_and_sql.html">Class 06: Data Science 1 — Pandas, SQL, Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_07/class_07_creating_a_network_from_sql_tables.html">Class 07: Data Science 2 — SQL for Network Construction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_08/class_08_communities1.html">Class 08: Clustering &amp; Community Detection 1 — Traditional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_09/class_09_communities2.html">Class 09: Clustering &amp; Community Detection 2 — Contemporary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_12/class_12_visualization_python.html">Class 12: Visualization 1 — Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_13/class_13_visualization_python.html">Class 13: Visualization 2 — Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_14/class_14_graph_machine_learning_1.html">Class 14: Graph Machine Learning 1</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Class 15: Graph Machine Learning 2</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/network-science-data-and-models/phys7332_fa25" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/network-science-data-and-models/phys7332_fa25/issues/new?title=Issue%20on%20page%20%2Fnotebooks/class_15/class_15_graph_ml_2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/class_15/class_15_graph_ml_2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Class 15: Graph Machine Learning 2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-of-today-s-class">Goals of today’s class:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings-and-neural-networks">Embeddings and Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings">Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tovec">*tovec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#on-training-neural-networks">On Training Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpus">GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-graph-neural-networks-gnns-do">What do graph neural networks (GNNs) do?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-learn-about-nodes-neighborhoods">How do we learn about nodes’ neighborhoods?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enter-permutation-invariant-functions">enter…permutation-invariant functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-permutation-invariant-function">What is a permutation-invariant function?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-care-about-permutation-invariance">Why do we care about permutation-invariance?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-core-functionalities-in-most-gnns">The three core functionalities in most GNNs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aggregate">AGGREGATE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#combine">COMBINE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#readout">READOUT</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-fun">Today’s Fun</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">Your Turn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-backpropagation-by-request">Appendix: Backpropagation (by request):</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-ingredients-do-we-need-to-do-backpropagation">What ingredients do we need to do backpropagation?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moving-right-along-tl-dr-for-those-who-hate-math">Moving right along/TL;DR (For those who hate math!!)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gcns-as-an-example-of-a-gnn">GCNs (as an example of a GNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-convolution-in-image-processing-world">What is a convolution in image processing world?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolutions">Graph Convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-formulation">Matrix Formulation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="class-15-graph-machine-learning-2">
<h1>Class 15: Graph Machine Learning 2<a class="headerlink" href="#class-15-graph-machine-learning-2" title="Link to this heading">#</a></h1>
<section id="goals-of-today-s-class">
<h2>Goals of today’s class:<a class="headerlink" href="#goals-of-today-s-class" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Understand what embeddings are and how neural networks work at a high level.</p></li>
<li><p>Explain what makes graph neural networks different from other kinds of neural networks (i.e. how do they learn on network data?)</p></li>
<li><p>Iterate on an example neural network model to learn about what affects its performance.</p></li>
</ol>
</section>
<section id="embeddings-and-neural-networks">
<h2>Embeddings and Neural Networks<a class="headerlink" href="#embeddings-and-neural-networks" title="Link to this heading">#</a></h2>
<p>TL;DR: We often represent our data as vectors, but these vectors can get really, really long. They also tend to be sparse. By learning <strong>embeddings</strong> that more concisely represent our data, we can learn more meaningful patterns in our data.</p>
<section id="embeddings">
<h3>Embeddings<a class="headerlink" href="#embeddings" title="Link to this heading">#</a></h3>
<p>When we talk about making <strong>embeddings</strong>, we’re talking about representing our data points or objects in our dataset as meaningful vectors. In graphs, and in natural language, we <em>could</em> represent nodes and words as sparse vectors. These might look like the sparse rows in an adjacency matrix, or they might look like one-hot representations whose length is equal to the cardinality of a language’s entire vocabulary. Of course, representations like these aren’t very useful when we think about looking at questions of similarity or really any sort of machine learning applications.</p>
<p>Enter <strong>embeddings</strong>. Embeddings can result from dimensionality reduction techniques like principal component analysis (PCA), but nowadays, when we think about embeddings, we tend to think about vectors that are created via training a neural network model to do a task.</p>
</section>
<section id="neural-networks">
<h3>Neural Networks<a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h3>
<p>(this section borrows heavily from the coverage of ML in Bagrow &amp; Ahn, section 16.2)</p>
<p>These can seem complicated to understand, but they, are at their heart, just doing a bunch of very parallelizable matrix operations. Here’s a picture of a simple <strong>feed-forward</strong> neural network, sourced from this handy <a class="reference external" href="https://medium.com/analytics-vidhya/in-depth-explanation-of-feedforward-in-neural-network-mathematically-448092216b63">Medium post</a>.</p>
<p><img alt="feed-forward NN" src="https://miro.medium.com/v2/resize:fit:3200/1*ycDUAMaxDYaHx7xl9pqEjg.png" /></p>
<p>Neural networks consist of sets of units (neurons) arranged in a sequence of layers. When we talk about the <strong>architecture</strong> of a neural network, we’re talking about how the units are arranged, how many of them there are, how many layers we have, and the number of units per layer. In a fully connected feed-forward network, each unit in each layer is connected to all the nodes in the layer after it. First, the input nodes (the green ones) receive the input data (this could be attributes of a product, for example). Data moves through the network in the picture from left to right.</p>
<p>The output of a particular unit is the activation function of the weighted sum of its inputs (<span class="math notranslate nohighlight">\(\sigma(\vec{w}^T \vec{x})\)</span>), where <span class="math notranslate nohighlight">\(\vec{w}\)</span> is the weights of the inputs for that unit, and <span class="math notranslate nohighlight">\(\vec{x}\)</span> is the inputs coming from the previous layer. <span class="math notranslate nohighlight">\(\sigma\)</span> is the nonlinear activation function; in this case, we’re probably thinking of the sigmoid function (pictured below &amp; sourced <a class="reference external" href="https://groups.csail.mit.edu/medg/hamish/medcomp3/sld007.htm">here</a>), but other activation functions, like <span class="math notranslate nohighlight">\(\tanh\)</span>, are possible as well (image sourced <a class="reference external" href="https://sebastianraschka.com/faq/docs/activation-functions.html">here</a>).</p>
<p><img alt="sigmoid" src="https://groups.csail.mit.edu/medg/hamish/medcomp3/img007.gif" /></p>
<p>Neural networks can learn nonlinear patterns in data because of these nonlinear activation functions. If they only used linear activation functions, they would be learning the weights for a weighted piecewise linear function on their inputs. In other words, neural networks are powerful precisely because they have many built-in nonlinearities.</p>
<p><img alt="activations" src="https://sebastianraschka.com/images/faq/activation-functions/activation-functions.png" /></p>
<p>The outputs we get from each unit in the middle layer then are passed to the final layer, and then we compare the final layer’s output to our desired output. Maybe the <span class="math notranslate nohighlight">\(\vec{y}\)</span> outputs in the picture are probabilities of the product belonging to a particular class, for example. Our task is to use a <strong>loss function</strong>, of which there are many, to express how far off we are from our correct answer. Once we’ve done that, we can use an algorithm called <strong>backpropagation</strong> to nudge the weights (<span class="math notranslate nohighlight">\(\vec{w}\)</span>) towards values that would be closer to producing the desired label.</p>
<p>Our choice of loss function is important here; what works well for classifying products may be terrible for predicting flight prices, for example. For a more complete primer on loss functions, check out <a class="reference external" href="https://www.geeksforgeeks.org/loss-functions-in-deep-learning/">this blog post</a>. Generally, your loss function and the tasks you’re setting up the neural network to learn will reflect what you want your neural network to do effectively. We might use a squared error loss function if we’re predicting flight prices, and we might use a cross-entropy loss function to quantify how incorrect our predictions are for a classifier. There are more complicated loss functions and task setups, one of which we’ll discuss in the next section.</p>
</section>
<section id="tovec">
<h3>*tovec<a class="headerlink" href="#tovec" title="Link to this heading">#</a></h3>
<section id="word2vec">
<h4>Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://arxiv.org/pdf/1301.3781">Word2Vec</a> was a pretty early natural language processing technique that was very exciting at the time. It trained a fairly simple neural network to do one of two tasks: Continuous Bag-of-Words (CBOW) or Skip-Gram. CBOW involves predicting which word is missing from a given context of surrounding words, and Skip-Gram involves doing the opposite: predicting the surrounding context given a specific word. The loss function here is cross-entropy loss; we use a softmax activation function to turn the outputs of the final layer into a probability distribution and then use cross-entropy loss to quantify how incorrect we were. Here’s some more info; the links are where we sourced our images for <a class="reference external" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/">softmax</a> and <a class="reference external" href="https://devopedia.org/word2vec">word2vec</a>.</p>
<p><img alt="word2vec tasks" src="https://devopedia.org/images/article/221/9279.1570465016.png" /></p>
<p><img alt="softmax + cross-entropy" src="https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png" /></p>
</section>
<section id="node2vec">
<h4>Node2Vec<a class="headerlink" href="#node2vec" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://arxiv.org/pdf/1607.00653">Node2Vec</a> follows similar logic to Word2Vec. We do random walks through a network and treat these random walks like sentences. Then we can do CBOW or Skip-Gram to obtain node embeddings. At each iteration, Node2Vec samples <span class="math notranslate nohighlight">\(n * r\)</span> random walks (with <span class="math notranslate nohighlight">\(r\)</span> walks starting at each node). It has hyperparameters that influence how much we revisit nodes (<span class="math notranslate nohighlight">\(p\)</span>) and how much we favor visiting close nodes versus nodes that are further from where we started (<span class="math notranslate nohighlight">\(q\)</span>).</p>
<p>When people around the time of Node2Vec were thinking about building node embeddings, they tended to consider homophily (nodes that are linked are likely to be similar) or structural equivalence (nodes with similarly structured neighborhoods are likely to be similar). Random walks that are closer to depth-first search tend to get further away from their start node, so they’ll have a better sense of the community structure of a given graph and we’ll get more information about node homophily (in theory). However, longer-range dependencies can be harder to make sense of and more complicated. If we instead do something more like a breadth-first search, we’re thinking more about structural equivalence – whose neighborhoods looks similar? This isn’t likely to explore more distant parts of the graph, and nodes will repeat themselves a lot in a given walk. So Grover &amp; Leskovec structured their random walks with hyperparameters <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> that allow us to figure out where on the BFS - DFS spectrum we want to end up.</p>
<p>Our non-normalized probability <span class="math notranslate nohighlight">\(\alpha(v, x) * w(v, x)\)</span> of visiting a given node <span class="math notranslate nohighlight">\(x\)</span> after having visited <span class="math notranslate nohighlight">\(v\)</span> in the previous step is:
$<span class="math notranslate nohighlight">\(
\alpha(v, x) = 
\begin{cases}
\frac{1}{p} &amp; if d(v, x) = 0 \\
1 &amp; if d(v, x) = 1 \\
\frac{1}{q} &amp; if d(v, x) = 2
\end{cases}
\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(d(i, j)\)</span> is the number of hops between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>Here’s an image from the original Node2Vec paper of the different outcomes of Node2Vec on a network generated from the novel <em>Les Misérables</em> with hyperparameter settings that lean more towards prioritizing homophily (closer to DFS) and structural equivalence (closer to BFS).</p>
<p><img alt="lesmis" src="../../_images/les_mis_node2vec.png" /></p>
<p>Node2Vec also incorporates <strong>negative sampling</strong>, where we only look at the outputs for a couple nodes that definitely <strong>shouldn’t</strong> be in an input node’s neighborhood. We make sure our outputs for those nodes are zero and do backpropagation accordingly; it’s possible to tune how many negative samples we do for each positive sample. Adding negative samples improves our model’s performance, and it’s relatively cheap to do because we’re not tuning all the weights in the model at once.</p>
<p>We can do a version of Node2Vec where we’re making embeddings for embeddings’ sake, as we do in this setting, or we can try to train a classifier on Node2Vec embeddings to predict labels for nodes. Here we’re going to train Node2Vec (without node or edge labels) on the Cambridge, MA road network dataset. Let’s see what happens! Our training code is borrowed from <code class="docutils literal notranslate"><span class="pre">pytorch-geometric</span></code> and can be found <a class="reference external" href="https://github.com/pyg-team/pytorch_geometric/blob/master/examples/node2vec.py">here</a>.</p>
<p>Notice that since we’re just trying to build embeddings to demonstrate Node2Vec’s capabilities, we’re not holding out any data right now. Node2Vec <em>does</em> sample negative examples as it trains, so that the model learns which contexts or nodes are correct and builds accurate embeddings. If we were training a link prediction model with Node2Vec, as you can choose to do at the end of this class, we would at minimum hold out some fraction of links to make sure our embeddings were of good quality and useful for the downstream link prediction task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">osmnx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ox</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">shapely</span><span class="w"> </span><span class="kn">import</span> <span class="n">Point</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">geopandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gpd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">contextily</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cx</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="mi">15</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">ox</span><span class="o">.</span><span class="n">graph_from_place</span><span class="p">(</span><span class="s1">&#39;Cambridge, Massachusetts, United States&#39;</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">Point</span><span class="p">((</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">]</span>
<span class="n">geo_df</span> <span class="o">=</span> <span class="n">gpd</span><span class="o">.</span><span class="n">GeoDataFrame</span><span class="p">(</span><span class="n">geometry</span> <span class="o">=</span> <span class="n">points</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="s1">&#39;EPSG:4326&#39;</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/781fde894367668f4747fdccb577f04c92b1ed3af0a3c9796e8b041d0ef43a98.png" src="../../_images/781fde894367668f4747fdccb577f04c92b1ed3af0a3c9796e8b041d0ef43a98.png" />
</div>
</div>
</section>
</section>
</section>
<section id="on-training-neural-networks">
<h2>On Training Neural Networks<a class="headerlink" href="#on-training-neural-networks" title="Link to this heading">#</a></h2>
<p>We’re going to use <a class="reference external" href="https://pytorch.org/"><code class="docutils literal notranslate"><span class="pre">pytorch</span></code></a> and <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/"><code class="docutils literal notranslate"><span class="pre">pytorch-geometric</span></code></a> to make our neural network construction relatively pain-free. Pytorch is a widely used package for building &amp; training neural networks. It’s compatible with CUDA-capable GPUs (CUDA is Nvidia’s deep learning framework that makes doing ML in GPUs possible).</p>
<section id="gpus">
<h3>GPUs<a class="headerlink" href="#gpus" title="Link to this heading">#</a></h3>
<p>GPUs, or <em>graphical processing units</em>, are great at doing a lot of tiny calculations at the same time. This is different from CPUs, which are good at doing a few big calculations at once (this is a vast oversimplification, but it’s good enough for an overview). Neural networks do a lot of vector and matrix multiplication (Remember those dot products from before? How do they scale up to a whole layer?) which is very easy to parallelize on a GPU. We’re using GPUs today to speed up our Node2Vec training, which would be pretty slow on a CPU.</p>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Link to this heading">#</a></h3>
<p>In the neural network context, <strong>Hyperparameters</strong> are parameters that control how we find the neural network’s weights (the non-hyper parameters, or just plain parameters). Examples of hyperparameters are learning rate, number of layers, choice of optimizer, layer size, and activation function(s). Finding good hyperparameters can take a lot of time, depending on the number and types of hyperparameters you’re playing with; platforms like <a class="reference external" href="https://wandb.ai/site">Weights and Biases</a> have sprung up to help developers tune hyperparameters for their neural network models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Node2Vec</span> 

<span class="c1"># giving nodes new indices, as Node2Vec expects indices to start at 0 and end at |N| - 1. </span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
    <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">node</span><span class="p">][</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>

<span class="c1"># build a tensor with all the edges in the graph. </span>
<span class="n">my_tensor</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">:</span>
    <span class="n">my_tensor</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="s1">&#39;idx&#39;</span><span class="p">],</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="s1">&#39;idx&#39;</span><span class="p">]])</span>

<span class="c1"># actually convert it to a torch Tensor -- </span>
<span class="c1"># this has to be a torch datatype or it will not play nicely with the GPU.</span>
<span class="n">edge_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="c1"># checking if we have a CUDA-capable GPU available.</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># constructing our Node2Vec model and giving it our edge list as input. </span>
<span class="c1"># note that you can tweak all of these parameters!</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Node2Vec</span><span class="p">(</span>
    <span class="n">edge_list</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">walk_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">walks_per_node</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">num_negative_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># we have 2 CPU cores requested by default in our sessions. </span>
<span class="c1"># (we use CPU cores to load our data).</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;linux&#39;</span> <span class="k">else</span> <span class="mi">0</span>

<span class="c1"># we use batches to work through our dataset without overloading the GPU.</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loader</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
<span class="c1"># you can play with optimizer choices; Adam and AdaGrad are popular choices.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SparseAdam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">):</span> <span class="c1"># an epoch refers to going over all the data once</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># set the model to training mode; </span>
    <span class="c1"># this lets it accumulate gradients and do backpropagation</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pos_rw</span><span class="p">,</span> <span class="n">neg_rw</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># zero out the gradients to start fresh</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">pos_rw</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">neg_rw</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span> 
        <span class="c1"># compute the loss (cross-entropy)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># do backpropagation</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># updates the parameters</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># keeps track of our total loss (it should decrease over time)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span> <span class="c1"># how wrong are we on average?</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="c1"># keep track of our progress</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cuda
Epoch: 001, Loss: 2.9328
Epoch: 002, Loss: 1.5111
Epoch: 003, Loss: 1.1077
Epoch: 004, Loss: 0.9537
Epoch: 005, Loss: 0.8816
Epoch: 006, Loss: 0.8436
Epoch: 007, Loss: 0.8215
Epoch: 008, Loss: 0.8074
Epoch: 009, Loss: 0.7978
Epoch: 010, Loss: 0.7916
Epoch: 011, Loss: 0.7870
Epoch: 012, Loss: 0.7841
Epoch: 013, Loss: 0.7816
Epoch: 014, Loss: 0.7800
Epoch: 015, Loss: 0.7788
Epoch: 016, Loss: 0.7777
Epoch: 017, Loss: 0.7773
Epoch: 018, Loss: 0.7766
Epoch: 019, Loss: 0.7760
Epoch: 020, Loss: 0.7755
Epoch: 021, Loss: 0.7754
Epoch: 022, Loss: 0.7751
Epoch: 023, Loss: 0.7749
Epoch: 024, Loss: 0.7744
Epoch: 025, Loss: 0.7745
Epoch: 026, Loss: 0.7743
Epoch: 027, Loss: 0.7739
Epoch: 028, Loss: 0.7739
Epoch: 029, Loss: 0.7737
Epoch: 030, Loss: 0.7734
Epoch: 031, Loss: 0.7733
Epoch: 032, Loss: 0.7731
Epoch: 033, Loss: 0.7730
Epoch: 034, Loss: 0.7729
Epoch: 035, Loss: 0.7727
Epoch: 036, Loss: 0.7724
Epoch: 037, Loss: 0.7721
Epoch: 038, Loss: 0.7720
Epoch: 039, Loss: 0.7719
Epoch: 040, Loss: 0.7716
Epoch: 041, Loss: 0.7714
Epoch: 042, Loss: 0.7714
Epoch: 043, Loss: 0.7712
Epoch: 044, Loss: 0.7710
Epoch: 045, Loss: 0.7706
Epoch: 046, Loss: 0.7705
Epoch: 047, Loss: 0.7703
Epoch: 048, Loss: 0.7701
Epoch: 049, Loss: 0.7700
Epoch: 050, Loss: 0.7698
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> 
<span class="c1"># set model to eval mode - this means it doesn&#39;t accumulate gradients</span>
<span class="c1"># and uses way less memory</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span> <span class="c1"># get embeddings</span>
<span class="n">z_cpu</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> 
<span class="c1"># put these on the CPU and detach it from the neural network. </span>
</pre></div>
</div>
</div>
</div>
<p>Now that we’ve created embeddings from the Cambridge street network, we’ll want to figure out how useful our embeddings actually are! To do so, we’ll use a clustering algorithm. Clustering can be useful for recommendation algorithms, pattern recognition, and more. We want to partition our dataset such that objects within groups are more similar to objects in the same group than other groups – so it’s similar logic to community detection.</p>
<p>K-means is a popular clustering algorithm that we’ll try out today; it partitions the dataset into <span class="math notranslate nohighlight">\(k\)</span> clusters, and each point is assigned to the cluster whose centroid (its mean) is closest to that point. Here’s a <a class="reference external" href="https://www.youtube.com/watch?v=R2e3Ls9H_fc">video</a> of K-means in action. It starts by randomly assigning centroids and then assigning each point to its closest centroid. Then, the centroid is recalculated (based on the mean of the points assigned to the cluster) and the process repeats until convergence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">contextily</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cx</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;tab20&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">adj_inv_length_kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">z_cpu</span><span class="p">)</span>
<span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_adj_inv_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">rgb2hex</span><span class="p">(</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adj_inv_length_kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_adj_inv_length&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a886147053f9fd216b0dc47b8b7387800946fd97397ba2d196c328ebf62a1171.png" src="../../_images/a886147053f9fd216b0dc47b8b7387800946fd97397ba2d196c328ebf62a1171.png" />
</div>
</div>
<p>That’s pretty cool! However, Node2Vec is just the beginning of graph machine learning. We’re now going to talk about graph neural networks – neural networks that take graph data as input and learn patterns in the graph data.</p>
</section>
</section>
<section id="what-do-graph-neural-networks-gnns-do">
<h2>What do graph neural networks (GNNs) do?<a class="headerlink" href="#what-do-graph-neural-networks-gnns-do" title="Link to this heading">#</a></h2>
<p>Bottom line: GNNs learn weight matrices, just like regular neural networks. These matrices transform node attributes or embeddings. They aggregate information about a node’s neighborhood in order to make the next round of node embeddings. These embeddings can then be used for interesting downstream tasks.</p>
<section id="applications">
<h3>Applications<a class="headerlink" href="#applications" title="Link to this heading">#</a></h3>
<p><strong>INTERACTIVE MOMENT:</strong>
Let’s think of some applications for GNNs. What would you use a GNN to do?</p>
<hr class="docutils" />
<p>Examples: protein-protein interaction prediction, fraud detection, or social network recommendations.</p>
</section>
</section>
<section id="how-do-we-learn-about-nodes-neighborhoods">
<h2>How do we learn about nodes’ neighborhoods?<a class="headerlink" href="#how-do-we-learn-about-nodes-neighborhoods" title="Link to this heading">#</a></h2>
<p>Nodes in networks notably have neighborhoods of variable size, but we want to represent all nodes with vectors of the same size. So an approach like word2vec might not work if we’re trying to aggregate information about a node’s neighborhood. Recall that we concatenated one-hot vectors representing the <span class="math notranslate nohighlight">\(k\)</span> words surrounding a word in a sentence to form a word’s context when training word2vec – but how do we know which <span class="math notranslate nohighlight">\(k\)</span> nodes ought to form the “context” of a node with far more than <span class="math notranslate nohighlight">\(k\)</span> neighbors?</p>
<section id="enter-permutation-invariant-functions">
<h3>enter…permutation-invariant functions<a class="headerlink" href="#enter-permutation-invariant-functions" title="Link to this heading">#</a></h3>
<section id="what-is-a-permutation-invariant-function">
<h4>What is a permutation-invariant function?<a class="headerlink" href="#what-is-a-permutation-invariant-function" title="Link to this heading">#</a></h4>
<p>Permutation-invariant functions take in multiple inputs (say, a list of inputs), and they produce the same output regardless of the order in which the inputs are given. So if <span class="math notranslate nohighlight">\(f(x, y, z) = f(y, z, x)\)</span>, and so on for all orderings of <span class="math notranslate nohighlight">\(x, y, z\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is permutation-invariant.</p>
</section>
<section id="why-do-we-care-about-permutation-invariance">
<h4>Why do we care about permutation-invariance?<a class="headerlink" href="#why-do-we-care-about-permutation-invariance" title="Link to this heading">#</a></h4>
<p>Permutation-invariance is really useful for incorporating information about a node’s neighborhood in a graph. For example, operations like the mean, maximum, sum, and minimum are all permutation-invariant. We can put in as many nodes’ attributes as we’d like, and our output will maintain the same dimensionality. It will also be insensitive to how we order the inputs, so we don’t have to worry about how to order data that doesn’t come with inherent order.</p>
</section>
</section>
<section id="the-three-core-functionalities-in-most-gnns">
<h3>The three core functionalities in most GNNs<a class="headerlink" href="#the-three-core-functionalities-in-most-gnns" title="Link to this heading">#</a></h3>
<section id="aggregate">
<h4>AGGREGATE<a class="headerlink" href="#aggregate" title="Link to this heading">#</a></h4>
<p>In order to pass information through a GNN, we first gather up our information about a node’s neighborhood – this might be a set of node embeddings from a previous layer or the nodes’ raw feature vectors. Then, we pass this set of vectors through a permutation-invariant function like MEAN. This aggregates our information about the node’s neighborhood into a vector of fixed length.</p>
</section>
<section id="combine">
<h4>COMBINE<a class="headerlink" href="#combine" title="Link to this heading">#</a></h4>
<p>Next, we need to update our node’s embedding. We might concatenate our neighborhood vector with our previous node embedding (or feature vector). Some GNNs will include a node in its own neighborhood during aggregation, thereby bypassing the COMBINE step. This gives us the embedding for the node that will be passed to the next layer.</p>
</section>
<section id="readout">
<h4>READOUT<a class="headerlink" href="#readout" title="Link to this heading">#</a></h4>
<p>Often we need something more than just node embeddings – we might need information about the whole graph, in which case we’ll need to apply a permutation-invariant function to our entire set of node embeddings produced by our last GNN layer, or we might need to pass individual node embeddings through some linear neural network layers to classify nodes, for example.</p>
</section>
</section>
</section>
<section id="today-s-fun">
<h2>Today’s Fun<a class="headerlink" href="#today-s-fun" title="Link to this heading">#</a></h2>
<p>Today we’re going to use a kind of graph neural network called a GCN (Graph Convolutional Network). I’ve added an in-depth mathematical description of GCNs to the Appendix of this notebook; I’d like to focus on the programming aspects of working with neural networks and graph data for today.</p>
<p>TL;DR: A <a class="reference external" href="https://arxiv.org/pdf/1609.02907">GCN</a> (Kipf &amp; Welling, 2017) is a type of graph neural network that aggregates nodes’ neighborhoods in a clever way. It uses insights from image processing to perform the AGGREGATE functionality. It scales nicely (as far as GNNs go), learns about graph structure <strong>and</strong> node features, and performs quite well on graphs with node features &amp; labels.</p>
<p>We’re going to practice training a GCN on the Cora dataset. Cora is a commonly used citation dataset; the task is to label ML publications by subject area based on links (citations) between publications.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">GCNConv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">CitationFull</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.loader</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomNodeSplit</span>

<span class="c1"># loading the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">CitationFull</span><span class="p">(</span><span class="s1">&#39;/courses/PHYS7332.202610/shared/data/&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cora&#39;</span><span class="p">)</span>

<span class="c1"># checking if the GPU is available; else use the CPU. </span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># yell if you don&#39;t see &#39;cuda&#39; here!</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GCN</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># we&#39;re making two GCN convolutional layers here!</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gcn1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> 
        <span class="c1"># the first one takes in the raw node features and outputs a vector of length 64.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gcn2</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="c1"># the second one takes in the output of gcn1 and outputs a vector of length 16.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>  
        <span class="c1"># then we make a one-hot vector with an entry for each class in the dataset.</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcn1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span> <span class="c1"># we use a ReLU activation function.</span>
        <span class="n">h2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gcn2</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span> <span class="c1"># this indicates how data moves through the network.</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h2</span><span class="p">,</span> <span class="n">z</span>

<span class="n">splits</span> <span class="o">=</span> <span class="n">RandomNodeSplit</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s1">&#39;train_rest&#39;</span><span class="p">,</span> <span class="n">num_val</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">num_test</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)(</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># this lets us make a mask on our dataset </span>
<span class="c1"># such that we&#39;re only training the model on a subset of nodes.</span>
<span class="c1"># we have a validation set that we look at each epoch to track our accuracy</span>
<span class="c1"># as well as a test set that we can use to look at our performance at the end of training.</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GCN</span><span class="p">()</span> <span class="c1">#instantiates our GCN</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># puts it on the GPU</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span> 
<span class="c1"># cross-entropy loss tells us how wrong given our final output </span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span> 
<span class="c1"># you can mess with the learning rate or choice of optimizer</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># keeps track of gradients; this is memory-intensive.</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># keep track of loss</span>
    <span class="n">tot_accuracy</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># keep track of </span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
        <span class="c1"># zero the gradient so we aren&#39;t accumulating them unnecessarily</span>
        <span class="n">h2</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span> 
        <span class="c1"># make sure we&#39;re putting our data on GPU</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">splits</span><span class="o">.</span><span class="n">train_mask</span><span class="p">],</span> <span class="n">batch</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)[</span><span class="n">splits</span><span class="o">.</span><span class="n">train_mask</span><span class="p">])</span>
        <span class="c1"># only do backpropagation based on nodes in the train set.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># this is the backpropagation step.</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># optimizers control how backpropagation goes. T</span>
        <span class="c1"># The fancier ones, like Adam, can adjust the learning rate </span>
        <span class="c1"># dynamically depending on the magnitude of the gradients.</span>
        <span class="c1"># AdaGrad can change the learning rate for each rate, so it&#39;s really fancy.</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># keep track of our total loss (cross-entropy)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># put the model in eval mode - don&#39;t accumulate gradients.</span>
    <span class="c1"># this saves memory!</span>
    <span class="n">val_h</span><span class="p">,</span> <span class="n">val_z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">dataset</span><span class="o">.</span><span class="n">edge_index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span> 
    <span class="c1"># run our dataset through the model</span>
    <span class="n">val_z</span> <span class="o">=</span> <span class="n">val_z</span><span class="p">[</span><span class="n">splits</span><span class="o">.</span><span class="n">val_mask</span><span class="p">]</span>
    <span class="c1"># look only at the validation set&#39;s vectors</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="n">val_z</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
    <span class="c1"># what predictions did we get for the classes?</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)[</span><span class="n">splits</span><span class="o">.</span><span class="n">val_mask</span><span class="p">]</span>
    <span class="n">tot_accuracy</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="c1"># how often were we right?</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tot_accuracy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cuda
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/courses/PHYS7332.202610/shared/ml-env-hopefully-working/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an &#39;InMemoryDataset&#39;. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 001, Loss: 4.2748, Accuracy: 0.0165
Epoch: 002, Loss: 4.1549, Accuracy: 0.0505
Epoch: 003, Loss: 3.9971, Accuracy: 0.1876
Epoch: 004, Loss: 3.7546, Accuracy: 0.1738
Epoch: 005, Loss: 3.5283, Accuracy: 0.1671
Epoch: 006, Loss: 3.3456, Accuracy: 0.2327
Epoch: 007, Loss: 3.1314, Accuracy: 0.2849
Epoch: 008, Loss: 2.9415, Accuracy: 0.3425
Epoch: 009, Loss: 2.7567, Accuracy: 0.3860
Epoch: 010, Loss: 2.5565, Accuracy: 0.4362
Epoch: 011, Loss: 2.3663, Accuracy: 0.4722
Epoch: 012, Loss: 2.1951, Accuracy: 0.4998
Epoch: 013, Loss: 2.0282, Accuracy: 0.5312
Epoch: 014, Loss: 1.8785, Accuracy: 0.5544
Epoch: 015, Loss: 1.7522, Accuracy: 0.5709
Epoch: 016, Loss: 1.6255, Accuracy: 0.5776
Epoch: 017, Loss: 1.5199, Accuracy: 0.5921
Epoch: 018, Loss: 1.4267, Accuracy: 0.5958
Epoch: 019, Loss: 1.3424, Accuracy: 0.6073
Epoch: 020, Loss: 1.2561, Accuracy: 0.6117
Epoch: 021, Loss: 1.1884, Accuracy: 0.6207
Epoch: 022, Loss: 1.1326, Accuracy: 0.6309
Epoch: 023, Loss: 1.0834, Accuracy: 0.6332
Epoch: 024, Loss: 1.0323, Accuracy: 0.6356
Epoch: 025, Loss: 0.9814, Accuracy: 0.6443
Epoch: 026, Loss: 0.9338, Accuracy: 0.6511
Epoch: 027, Loss: 0.8880, Accuracy: 0.6544
Epoch: 028, Loss: 0.8490, Accuracy: 0.6561
Epoch: 029, Loss: 0.8212, Accuracy: 0.6568
Epoch: 030, Loss: 0.7861, Accuracy: 0.6642
Epoch: 031, Loss: 0.7516, Accuracy: 0.6628
Epoch: 032, Loss: 0.7247, Accuracy: 0.6612
Epoch: 033, Loss: 0.6996, Accuracy: 0.6598
Epoch: 034, Loss: 0.6705, Accuracy: 0.6655
Epoch: 035, Loss: 0.6443, Accuracy: 0.6585
Epoch: 036, Loss: 0.6261, Accuracy: 0.6618
Epoch: 037, Loss: 0.6055, Accuracy: 0.6645
Epoch: 038, Loss: 0.5796, Accuracy: 0.6686
Epoch: 039, Loss: 0.5566, Accuracy: 0.6686
Epoch: 040, Loss: 0.5352, Accuracy: 0.6699
Epoch: 041, Loss: 0.5160, Accuracy: 0.6706
Epoch: 042, Loss: 0.4985, Accuracy: 0.6713
Epoch: 043, Loss: 0.4841, Accuracy: 0.6676
Epoch: 044, Loss: 0.4713, Accuracy: 0.6716
Epoch: 045, Loss: 0.4548, Accuracy: 0.6679
Epoch: 046, Loss: 0.4356, Accuracy: 0.6706
Epoch: 047, Loss: 0.4190, Accuracy: 0.6709
Epoch: 048, Loss: 0.4081, Accuracy: 0.6686
Epoch: 049, Loss: 0.3965, Accuracy: 0.6733
Epoch: 050, Loss: 0.3822, Accuracy: 0.6726
</pre></div>
</div>
</div>
</div>
</section>
<section id="your-turn">
<h2>Your Turn<a class="headerlink" href="#your-turn" title="Link to this heading">#</a></h2>
<p>For this Your Turn section, I want you to do one or more of the following:</p>
<ol class="arabic simple">
<li><p>Figure out how to make a GCN model with an adjustable number of layers (e.g. <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">GCN(3)</span></code> should give me a model that has three GCN layers). Try training the model with several different numbers of layers. Tell me how the performance changes as the number of layers increases/decreases. Optionally, look at the embeddings that the model produces and tell me if their quality changes.</p></li>
<li><p>The <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv">GCNConv</a> layer takes several different keyword arguments that are its own (e.g. <code class="docutils literal notranslate"><span class="pre">improved</span></code>, <code class="docutils literal notranslate"><span class="pre">add_self_loops</span></code>, <code class="docutils literal notranslate"><span class="pre">normalize</span></code>) or can be inherited from the <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing">MessagePassing</a> class in <code class="docutils literal notranslate"><span class="pre">torch-geometric</span></code>, as GCNConv is a message-passing GNN layer. The <code class="docutils literal notranslate"><span class="pre">MessagePassing</span></code> arguments include a choice of aggregation function and the ability to change the flow of message-passing. Mess with these keyword arguments and keep track of the accuracy and loss as training proceeds for a few settings of, say, aggregation function. Plot your accuracy and/or loss over the course of training for several different settings of the parameter you chose to vary. What do you notice? Why do you think this is the case?</p></li>
<li><p>Look at the different choices of convolutional layers available <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers">here</a>. Choose a couple different types of convolutional layers and build models with those layers. Which do well on this dataset? Which do worse? Why do you think that is?</p></li>
</ol>
</section>
<section id="appendix-backpropagation-by-request">
<h2>Appendix: Backpropagation (by request):<a class="headerlink" href="#appendix-backpropagation-by-request" title="Link to this heading">#</a></h2>
<p>At a very high level, backpropagation is how we adjust our weights, going back from the output layer (and our loss function) all the way back to the weights from the input layer.</p>
<p>It involves computing a bunch of partial derivatives (gradients) and adjusting our weights/biases (the parameters we’re learning in our neural network) according to the relationship between the gradients and the loss.</p>
<section id="what-ingredients-do-we-need-to-do-backpropagation">
<h3>What ingredients do we need to do backpropagation?<a class="headerlink" href="#what-ingredients-do-we-need-to-do-backpropagation" title="Link to this heading">#</a></h3>
<p>First, we need a loss function. Our loss function (or cost function) needs to be differentiable with respect to the weights and biases we use in the network. Our loss also has to be expressed as a function of the input and our weights and biases. For example, let’s look at a toy example with one hidden layer and a mean squared error (MSE) loss function.</p>
<p><img alt="feed-forward NN" src="https://miro.medium.com/v2/resize:fit:3200/1*ycDUAMaxDYaHx7xl9pqEjg.png" /></p>
<p>The simplified output <span class="math notranslate nohighlight">\(g(\vec{x})\)</span> of our neural network with input <span class="math notranslate nohighlight">\(\vec{x}\)</span>, given weight matrices <span class="math notranslate nohighlight">\(W^{(1)}\)</span> and <span class="math notranslate nohighlight">\(W^{(2)}\)</span> and generic activation functions <span class="math notranslate nohighlight">\(\sigma^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\sigma^{(2)}\)</span>, is</p>
<div class="math notranslate nohighlight">
\[g(x) = \sigma^{(2)}(W^{(2)} \sigma^{(1)}(W^{(1)} \vec{x}))\]</div>
<p>Our loss function, with ground truth <span class="math notranslate nohighlight">\(\vec{z}\)</span>, is <span class="math notranslate nohighlight">\(\lvert \lvert g(\vec{x}) - \vec{z} \rvert \rvert\)</span>. In the generalized sense, we can use a generic loss function <span class="math notranslate nohighlight">\(C(g(\vec{x}), \vec{z})\)</span>.</p>
<p>Next, we need partial derivatives and the chain rule!</p>
<p>This is how backpropagation adjusts the weights – we compute the partial derivative of our cost function <span class="math notranslate nohighlight">\(C\)</span> with respect to one weight from node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span> in the <span class="math notranslate nohighlight">\(2^{nd}\)</span> matrix of weights:</p>
<div class="math notranslate nohighlight">
\[\frac{\delta C}{\delta w^{(2)}_{ij}} = \frac{\delta C}{\delta y_j} \frac{\delta y_j}{\delta w^{(2)}_{ij}}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(y_j\)</span> is the <span class="math notranslate nohighlight">\(j^{th}\)</span> output of our network (in the output layer).</p>
<div class="math notranslate nohighlight">
\[y_j = \sigma^{(2)}(\sum_{i}w^{(2)}_{ij} * h_i)\]</div>
<p>In other words, we’re passing the dot product of row <span class="math notranslate nohighlight">\(j\)</span> of <span class="math notranslate nohighlight">\(W^{(2)}\)</span> and <span class="math notranslate nohighlight">\(\vec{h}\)</span>, our hidden layer’s output, through a sigmoid function. Let’s call <span class="math notranslate nohighlight">\(\sum_{i}w^{(2)}_{ij} * h_i\)</span> <span class="math notranslate nohighlight">\(o_j\)</span>, and let’s expand our partial derivative expression using the chain rule once more.</p>
<div class="math notranslate nohighlight">
\[\frac{\delta C}{\delta w^{(2)}_{ij}} = \frac{\delta C}{\delta y_j} \frac{\delta y_j}{o_j} \frac{o_j}{\delta w^{(2)}_{ij}}\]</div>
<p>What are we doing here? We’re tracing how our specific weight <span class="math notranslate nohighlight">\(w^{(2)}_{ij}\)</span> affects our computed loss for a particular input (or batch of inputs).</p>
<p>We know that <span class="math notranslate nohighlight">\(\frac{\delta y_j}{o_j}\)</span> is the partial derivative of the activation function <span class="math notranslate nohighlight">\(\sigma^{(2)}\)</span>.</p>
<p>Additionally, we know that <span class="math notranslate nohighlight">\(\frac{o_j}{\delta w^{(2)}_{ij}}\)</span> is</p>
<div class="math notranslate nohighlight">
\[\frac{\delta}{\delta w^{(2)}_{ij}}\sum_{k}w^{(2)}_{kj}h_k\]</div>
<p>Only one term in this sum relies on <span class="math notranslate nohighlight">\(w^{(2)}_{ij}\)</span> – that’s <span class="math notranslate nohighlight">\(w^{(2)}_{ij} h_i\)</span>. This means this part of our partial derivative reduces to</p>
<div class="math notranslate nohighlight">
\[\frac{o_j}{\delta w^{(2)}_{ij}} = h_i\]</div>
<p>Now let’s look at <span class="math notranslate nohighlight">\(\frac{\delta y_j}{h_j}\)</span>. Let’s say we’re using a sigmoid activation function; in this case, this part of our partial derivative is</p>
<div class="math notranslate nohighlight">
\[\frac{\delta}{\delta h_j}\sigma(h_j) = \sigma(h_j) (1 - \sigma(h_j)) = y_j * (1 - y_j)\]</div>
<p>If we’re using MSE for the loss function <span class="math notranslate nohighlight">\(C\)</span> and <span class="math notranslate nohighlight">\(\vec{z}\)</span> is our ground truth answer,</p>
<p>$<span class="math notranslate nohighlight">\(\frac{\delta C}{\delta y_j} = 2 (z_j - y_j)\)</span>$.</p>
<p>Therefore, the gradient of our loss with respect to <span class="math notranslate nohighlight">\(w^{(2)}_{ij}\)</span> is</p>
<p>$<span class="math notranslate nohighlight">\(\frac{\delta C}{\delta w^{(2)}_{ij}} = 2 (z_j - y_j) * y_j * (1 - y_j) * h_i\)</span>$.</p>
</section>
<section id="moving-right-along-tl-dr-for-those-who-hate-math">
<h3>Moving right along/TL;DR (For those who hate math!!)<a class="headerlink" href="#moving-right-along-tl-dr-for-those-who-hate-math" title="Link to this heading">#</a></h3>
<p>We take partial derivatives with the chain rule to figure out how much our loss function changes with respect to a particular parameter (like a weight or bias) in the neural network.</p>
<p>Then we can change that specific weight with this information. We usually have a learning rate <span class="math notranslate nohighlight">\(\eta\)</span> (or an optimizer that governs the learning rate, which is fancier) that tells us how much to change a weight/bias with respect to our computed gradient.</p>
<p>$<span class="math notranslate nohighlight">\(\delta w^{(2)}_{ij} = \eta \frac{\delta C}{\delta w^{(2)}_{ij}}\)</span>$.</p>
<p>We don’t want to update our parameters too much based on any one example, which is why the learning rate tends to be pretty small (much less than 1) and optimizers will lower the learning rate as training goes on and the model gets better at its task.</p>
<p>Let’s review how backpropagation works by watching <a class="reference external" href="https://www.youtube.com/watch?v=GlcnxUlrtek&amp;amp;list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&amp;amp;index=4">this video</a>.</p>
</section>
</section>
<section id="gcns-as-an-example-of-a-gnn">
<h2>GCNs (as an example of a GNN)<a class="headerlink" href="#gcns-as-an-example-of-a-gnn" title="Link to this heading">#</a></h2>
<p>Material in this section relies heavily on Maxime Labonne’s <a class="reference external" href="https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95">blog post</a> in <em>Towards Data Science</em> and Thomas Kipf’s <a class="reference external" href="https://tkipf.github.io/graph-convolutional-networks/">blog post</a> on his GitHub website.</p>
<p>A <a class="reference external" href="https://arxiv.org/pdf/1609.02907">GCN</a> (Kipf &amp; Welling, 2017) is a type of graph neural network that aggregates nodes’ neighborhoods in a clever way. It uses insights from image processing to perform the AGGREGATE functionality. It scales nicely (as far as GNNs go), learns about graph structure <strong>and</strong> node features, and performs quite well on graphs with node features &amp; labels.</p>
<section id="what-is-a-convolution-in-image-processing-world">
<h3>What is a convolution in image processing world?<a class="headerlink" href="#what-is-a-convolution-in-image-processing-world" title="Link to this heading">#</a></h3>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is used in image processing to blur, enhance, sharpen, or detect edges in an image. It’s a small matrix (relative to the size of the image) that is applied to each pixel in the image <strong>and its neighbors within a certain distance</strong>.</p>
<p>The generic equation for a kernel is this, where <span class="math notranslate nohighlight">\(\omega\)</span> is the kernel matrix, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> indicate the dimensions of the kernel, and <span class="math notranslate nohighlight">\(f(x, y)\)</span> is the <span class="math notranslate nohighlight">\((x, y)^{th}\)</span> pixel of the image:</p>
<div class="math notranslate nohighlight">
\[g(x, y) = \sum_{i=-a}^{a} \sum^{b}_{j=-b} \omega(i, j) f(x-i, y-j)\]</div>
<p>Here, <span class="math notranslate nohighlight">\(g(x, y)\)</span> is the <span class="math notranslate nohighlight">\((x, y)^{th}\)</span> pixel of the output image.</p>
<p>Here’s a visual example of a convolution matrix being applied to a single pixel (from <a class="reference external" href="https://zaforf.github.io/isp/study/CNN/">this primer on CNNs by Zafir Nasim</a>):
<img alt="" src="https://anhreynolds.com/img/cnn.png" /></p>
</section>
<section id="graph-convolutions">
<h3>Graph Convolutions<a class="headerlink" href="#graph-convolutions" title="Link to this heading">#</a></h3>
<p>You might say, cool, that’s neat, but how does that apply to graphs? First of all, graph neighborhoods are not rectangular in shape, and graphs notably have degree <em>distributions</em> - not every node has the same number of neighbors (far from it)!</p>
<p>Let’s tackle what happens in GCNs at the node level first. We’ll look at how we create our first embedding for node <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(h_{i}^{(1)}\)</span>.</p>
<p>We know we need to merge our node features with those of our neighbors, so we define a node <span class="math notranslate nohighlight">\(i\)</span>’s neighborhood here as <span class="math notranslate nohighlight">\(i\)</span>’s neighbors plus <span class="math notranslate nohighlight">\(i\)</span> itself. We’ll denote this as <span class="math notranslate nohighlight">\(\tilde{N_i}\)</span>.</p>
<p>In the simplest case, we could create a weight matrix <span class="math notranslate nohighlight">\(W_i\)</span> and multiply each node <span class="math notranslate nohighlight">\(j\)</span>’s features <span class="math notranslate nohighlight">\(x_j\)</span> by <span class="math notranslate nohighlight">\(W_i\)</span>, then sum them:</p>
<div class="math notranslate nohighlight">
\[h_{i}^{(1)} = \sum_{j \in \tilde{N_i}} W^{(1)} x_j\]</div>
<p>This seems neat, but there’s a small problem.</p>
<p><strong>INTERACTIVE MOMENT</strong>: Nodes in graphs notably don’t all have the same degree. What’s going to happen to the vectors of high-degree nodes as compared to those of low-degree nodes right now? How might we fix this?</p>
<hr class="docutils" />
<p>Spoiler alert: we’re going to divide by <span class="math notranslate nohighlight">\(k_i\)</span>, the degree of node <span class="math notranslate nohighlight">\(i\)</span>. This keeps vector magnitudes around the same-ish size.</p>
<div class="math notranslate nohighlight">
\[h_{i}^{(1)} = \frac{1}{k_i}\sum_{j \in \tilde{N_i}} W^{(1)} x_j\]</div>
<p>However, there’s one more improvement we can make. Kipf and Welling noticed that features from high-degree nodes tended to propagate through the network more easily than those from low-degree nodes. They therefore up-weight the lower-degree nodes’ contributions in the following way:</p>
<div class="math notranslate nohighlight">
\[h_{i}^{(1)} = \sum_{j \in \tilde{N_i}} \frac{1}{\sqrt{k_i}}\frac{1}{\sqrt{k_j}} W^{(1)} x_j\]</div>
<p><strong>INTERACTIVE MOMENT</strong>: Why does this work?</p>
<section id="matrix-formulation">
<h4>Matrix Formulation<a class="headerlink" href="#matrix-formulation" title="Link to this heading">#</a></h4>
<p>There’s also a neat way we can formulate this as a matrix multiplication. Here, <span class="math notranslate nohighlight">\(\hat{A}\)</span> is the adjacency matrix with self-loops added, and <span class="math notranslate nohighlight">\(\hat{D}\)</span> is <span class="math notranslate nohighlight">\(\hat{A}\)</span>’s degree matrix (i.e. <span class="math notranslate nohighlight">\(A + I\)</span>). <span class="math notranslate nohighlight">\(H^{(l)}\)</span> is the matrix of node embeddings coming into layer <span class="math notranslate nohighlight">\(l\)</span>, and <span class="math notranslate nohighlight">\(W^{(l)}\)</span> is the weight matrix of layer <span class="math notranslate nohighlight">\(l\)</span>:</p>
<p>$<span class="math notranslate nohighlight">\(f(H^{(l)}, A) = \sigma(\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})\)</span>$.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/class_15"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../class_14/class_14_graph_machine_learning_1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Class 14: Graph Machine Learning 1</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-of-today-s-class">Goals of today’s class:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings-and-neural-networks">Embeddings and Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings">Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tovec">*tovec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#on-training-neural-networks">On Training Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpus">GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-graph-neural-networks-gnns-do">What do graph neural networks (GNNs) do?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-learn-about-nodes-neighborhoods">How do we learn about nodes’ neighborhoods?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enter-permutation-invariant-functions">enter…permutation-invariant functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-permutation-invariant-function">What is a permutation-invariant function?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-care-about-permutation-invariance">Why do we care about permutation-invariance?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-core-functionalities-in-most-gnns">The three core functionalities in most GNNs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aggregate">AGGREGATE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#combine">COMBINE</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#readout">READOUT</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-fun">Today’s Fun</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">Your Turn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-backpropagation-by-request">Appendix: Backpropagation (by request):</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-ingredients-do-we-need-to-do-backpropagation">What ingredients do we need to do backpropagation?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moving-right-along-tl-dr-for-those-who-hate-math">Moving right along/TL;DR (For those who hate math!!)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gcns-as-an-example-of-a-gnn">GCNs (as an example of a GNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-convolution-in-image-processing-world">What is a convolution in image processing world?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolutions">Graph Convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-formulation">Matrix Formulation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Brennan Klein
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>