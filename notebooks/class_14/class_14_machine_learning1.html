
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Class 14: Machine Learning 1: Color-Coding Cambridge &#8212; PHYS 7332: Network Science Data &amp; Models (Fall 2025)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/class_14/class_14_machine_learning1';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
  
    <p class="title logo__title">PHYS 7332: Network Science Data & Models (Fall 2025)</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    PHYS 7332: Network Science Data & Models – Fall 2025
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../class_00/class_00_github_computing_setup.html">Class 00: Introduction and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_01/class_01_python_refresher.html">Class 01: Python Refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_02/class_02_networkx1.html">Class 02: Introduction to Networkx 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_03/class_03_networkx2.html">Class 03: Introduction to Networkx 2</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/network-science-data-and-models/phys7332_fa25" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/network-science-data-and-models/phys7332_fa25/issues/new?title=Issue%20on%20page%20%2Fnotebooks/class_14/class_14_machine_learning1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/class_14/class_14_machine_learning1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Class 14: Machine Learning 1: Color-Coding Cambridge</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-of-today-s-class">Goals of today’s class:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-unsupervised-machine-learning">What is Unsupervised Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#old-unsupervised-ml">“Old” Unsupervised ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-adventure">Today’s Adventure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering">K-Means Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-based-clustering">Network-Based Clustering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering">Spectral Clustering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-we-doing-it-s-dimensionality-reduction">What Are We Doing? It’s Dimensionality Reduction!</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#newer-unsupervised-ml">Newer Unsupervised ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings">Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tovec">*tovec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#on-training-neural-networks">On Training Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpus">GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-validation-test-sets">Train/Validation/Test Sets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn-choose-your-own-adventure">Your Turn: Choose Your Own Adventure!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perturbation-sensitivity">Perturbation Sensitivity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#link-prediction">Link Prediction</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="class-14-machine-learning-1-color-coding-cambridge">
<h1>Class 14: Machine Learning 1: Color-Coding Cambridge<a class="headerlink" href="#class-14-machine-learning-1-color-coding-cambridge" title="Link to this heading">#</a></h1>
<section id="goals-of-today-s-class">
<h2>Goals of today’s class:<a class="headerlink" href="#goals-of-today-s-class" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Take a whirlwind tour of the early(ish) history of unsupervised machine learning at a high level.</p></li>
<li><p>Learn what embeddings are and why they’re useful</p></li>
<li><p>Discuss the Node2Vec paper and try out Node2Vec on a medium-sized dataset.</p></li>
</ol>
</section>
<section id="what-is-unsupervised-machine-learning">
<h2>What is Unsupervised Machine Learning?<a class="headerlink" href="#what-is-unsupervised-machine-learning" title="Link to this heading">#</a></h2>
<p>Supervised machine learning is the practice of building a model when we have training data – the information that we use to inform our model how the world works – that has correct ground truth answers. When you’re building a linear regression or a Naive Bayes classifier (for example), you’re doing supervised machine learning.</p>
<p>However, there are some times when we don’t have access to exact ground truths about desired outcomes, and we have to build insights with our dataset anyway. That’s where unsupervised machine learning comes in. When we do unsupervised ML, we figure out what patterns are present in unlabeled data. Clustering is one type of unsupervised machine learning technique; when we cluster a dataset, we’re learning to recognize similar kinds of data points. Generating new examples is also (kind of) an unsupervised ML task. Training GPT and its ilk to produce language requires massive amounts of scraped text that doesn’t have “answers” attached to it per se. Humans aren’t going through and labeling the terabytes of data that GPT is trained on (before it is fine-tuned for specific tasks). Instead, at a high level, GPT and friends are trained to produce text that we can’t tell comes from a computer – it should “look like” the text that showed up in the massive amounts of data it was trained on. Once GPT has been trained in this fashion, it is also fine-tuned for specific tasks – this is where human labeling and direction <em>do</em> come in.</p>
</section>
<section id="old-unsupervised-ml">
<h2>“Old” Unsupervised ML<a class="headerlink" href="#old-unsupervised-ml" title="Link to this heading">#</a></h2>
<p>Today we’re going to start by going through some of the early ways people thought about unsupervised machine learning.</p>
<section id="clustering">
<h3>Clustering<a class="headerlink" href="#clustering" title="Link to this heading">#</a></h3>
<p>As mentioned above, one thing we do a lot in unsupervised settings is finding examples that are similar to each other. This can be useful for recommendation algorithms, pattern recognition, and more. We want to partition our dataset such that objects within groups are more similar to objects in the same group than other groups – so it’s similar logic to community detection.</p>
<p>K-means is a popular clustering algorithm that we’ll try out today; it partitions the dataset into <span class="math notranslate nohighlight">\(k\)</span> clusters, and each point is assigned to the cluster whose centroid (its mean) is closest to that point. Here’s a <a class="reference external" href="https://www.youtube.com/watch?v=R2e3Ls9H_fc">video</a> of K-means in action. It starts by randomly assigning centroids and then assigning each point to its closest centroid. Then, the centroid is recalculated (based on the mean of the points assigned to the cluster) and the process repeats until convergence.</p>
<p>When you’re clustering data, deciding on a value of <span class="math notranslate nohighlight">\(k\)</span> can be tricky; there are information theoretic ways to figure out what <span class="math notranslate nohighlight">\(k\)</span> should be for some clustering algorithms, or you can eyeball it using the “elbow method.” Here, you calculate the sum of squared error, also known as the “inertia” (how far is each data point, on average, from its centroid?) for a range of values of <span class="math notranslate nohighlight">\(k\)</span>. Then you plot the values and see where they start to decrease more slowly - this is the “elbow” of the graph and your optimal value of <span class="math notranslate nohighlight">\(k\)</span>. Here’s a picture of the elbow method in action (sourced from [Analytics Vidhya[(<a class="reference external" href="https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/">https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/</a>)).</p>
<p><img alt="elbow method" src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/image-24.png" /></p>
</section>
</section>
<section id="today-s-adventure">
<h2>Today’s Adventure<a class="headerlink" href="#today-s-adventure" title="Link to this heading">#</a></h2>
<p>Today we’re going to look at nodes in a street network for the city of Cambridge, MA. First, we’ll cluster them based on their locations in space using k-means. Then we’ll try to figure out if there are other informative ways to partition the city map.</p>
<p>First, let’s visualize the dataset. What are we dealing with here?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">osmnx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ox</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">shapely</span><span class="w"> </span><span class="kn">import</span> <span class="n">Point</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">geopandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gpd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">contextily</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cx</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="mi">15</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">ox</span><span class="o">.</span><span class="n">graph_from_place</span><span class="p">(</span><span class="s1">&#39;Cambridge, Massachusetts, United States&#39;</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">Point</span><span class="p">((</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">]</span>
<span class="n">geo_df</span> <span class="o">=</span> <span class="n">gpd</span><span class="o">.</span><span class="n">GeoDataFrame</span><span class="p">(</span><span class="n">geometry</span> <span class="o">=</span> <span class="n">points</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="s1">&#39;EPSG:4326&#39;</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span><span class="w"> </span><span class="nn">osmnx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ox</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">from</span><span class="w"> </span><span class="nn">shapely</span><span class="w"> </span><span class="kn">import</span> <span class="n">Point</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span><span class="w"> </span><span class="nn">geopandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gpd</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;osmnx&#39;
</pre></div>
</div>
</div>
</div>
<section id="k-means-clustering">
<h3>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Link to this heading">#</a></h3>
<p>Next, we’ll run k-means clustering on the dataset (specifically the spatial locations of the nodes) using the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> implementation. Feel free to play with the number of clusters or any other parameters noted in the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">documentation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">mtx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">])</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">mtx</span><span class="p">)</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;tab20&#39;</span><span class="p">)</span>
<span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">rgb2hex</span><span class="p">(</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/141b4124401da282847852dd77ba918f3a235981e9b04a998534163a54cce149.png" src="../../_images/141b4124401da282847852dd77ba918f3a235981e9b04a998534163a54cce149.png" />
</div>
</div>
</section>
</section>
<section id="network-based-clustering">
<h2>Network-Based Clustering<a class="headerlink" href="#network-based-clustering" title="Link to this heading">#</a></h2>
<p>What happens if we now try to cluster our network based on its adjacency matrix? We can use k-means again to see what happens!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">,</span><span class="n">d</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">d</span><span class="p">[</span><span class="s1">&#39;inverse_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;length&#39;</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;inverse_length&#39;</span><span class="p">)</span>

<span class="n">A</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">A</span><span class="o">.</span><span class="n">indptr</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">adj_inv_length_kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;tab20&#39;</span><span class="p">)</span>
<span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_adj_inv_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">rgb2hex</span><span class="p">(</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adj_inv_length_kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_adj_inv_length&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5f7ecd0ee69db24eebe35b1e9a0d7c15c98e3d0ac75bf55421634ecda9ca613d.png" src="../../_images/5f7ecd0ee69db24eebe35b1e9a0d7c15c98e3d0ac75bf55421634ecda9ca613d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">collections</span>
<span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">adj_inv_length_kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Counter({19: 16754,
         7: 4,
         1: 3,
         0: 3,
         8: 2,
         13: 1,
         2: 1,
         4: 1,
         12: 1,
         17: 1,
         14: 1,
         3: 1,
         9: 1,
         6: 1,
         15: 1,
         5: 1,
         16: 1,
         11: 1,
         18: 1,
         10: 1})
</pre></div>
</div>
</div>
</div>
<p>Okay, this method didn’t work great for this dataset. Why might that be? First of all, k-means assumes spherical-ish data. It doesn’t work great for data that’s high-dimensional, sparse, and not linearly separable, all of which likely applies to our adjacency matrix. So if we want to incorporate network structure into our clustering of Cambridge, we’re going to have to dig a bit deeper.</p>
</section>
<section id="spectral-clustering">
<h2>Spectral Clustering<a class="headerlink" href="#spectral-clustering" title="Link to this heading">#</a></h2>
<p>In spectral clustering, we start off (conveniently enough) with an adjacency (or similarity) matrix, <span class="math notranslate nohighlight">\(A\)</span>. We then calculate the Laplacian <span class="math notranslate nohighlight">\(L\)</span> of <span class="math notranslate nohighlight">\(A\)</span>. The Laplacian is defined as <span class="math notranslate nohighlight">\(D - A\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the degree matrix (each node’s degree is on the diagonal) and <span class="math notranslate nohighlight">\(A\)</span> is our adjacency matrix (or matrix of weights, if we’re using a weighted graph). Next, we calculate the first <span class="math notranslate nohighlight">\(k\)</span> eigenvectors of the Laplacian and make a matrix consisting of those first <span class="math notranslate nohighlight">\(k\)</span> eigenvectors. Each node <span class="math notranslate nohighlight">\(i\)</span> in the graph is now represented by a vector of length <span class="math notranslate nohighlight">\(k\)</span>. We can then cluster these shorter vectors using k-means and find our cluster labels.</p>
<p>One way we can think about spectral clustering is that we’re doing random walks on the graph represented by <span class="math notranslate nohighlight">\(A\)</span>. We want to define our clusters such that a random walker will stay for a long time in a particular cluster and won’t jump between clusters very often. We can also think of it as obtaining minimal graph cuts between partitions (see <a class="reference external" href="https://people.csail.mit.edu/dsontag/courses/ml14/notes/Luxburg07_tutorial_spectral_clustering.pdf">this article</a> by von Luxberg (2007) for more information).</p>
<p>Let’s see how spectral clustering does on our city network!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpectralClustering</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">,</span><span class="n">d</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">d</span><span class="p">[</span><span class="s1">&#39;inverse_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;length&#39;</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adjacency_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;inverse_length&#39;</span><span class="p">)</span>

<span class="n">A</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">A</span><span class="o">.</span><span class="n">indptr</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">spectral</span> <span class="o">=</span> <span class="n">SpectralClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">affinity</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;tab20&#39;</span><span class="p">)</span>
<span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_spectral&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">rgb2hex</span><span class="p">(</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">spectral</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_spectral&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/courses/PHYS7332.202510/shared/sad-ml/lib/python3.10/site-packages/sklearn/manifold/_spectral_embedding.py:310: UserWarning: Array is not symmetric, and will be converted to symmetric by average with its transpose.
  adjacency = check_symmetric(adjacency)
</pre></div>
</div>
<img alt="../../_images/1bd98c1222dddfbce229d0242ff0c1ac131fb39126f74d87c5dbd596a1589849.png" src="../../_images/1bd98c1222dddfbce229d0242ff0c1ac131fb39126f74d87c5dbd596a1589849.png" />
</div>
</div>
<p>This partition is somewhat different from the k-means partition in that it separates out MIT’s campus very neatly from the rest of Cambridge. The cluster boundaries here are also not necessarily polygons; k-means does something called <a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi tesselation</a>, where it partitions a space into cells, each of which contains all the points that are closer to a particular centroid than to any other centroid.</p>
<section id="what-are-we-doing-it-s-dimensionality-reduction">
<h3>What Are We Doing? It’s Dimensionality Reduction!<a class="headerlink" href="#what-are-we-doing-it-s-dimensionality-reduction" title="Link to this heading">#</a></h3>
<p>When we have high-dimensional data (like an adjacency matrix or user purchasing records), sometimes we want to reduce its dimensionality to better understand what we’re dealing with. There are several ways to do this – getting the eigenvectors of the Laplacian is one way, but we could also run <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> (principal component analysis) or other linear algebra-based dimensionality reduction algorithms. We’ll talk about fancier ways to reduce our data’s dimensionality in the next section.</p>
</section>
</section>
<section id="newer-unsupervised-ml">
<h2>Newer Unsupervised ML<a class="headerlink" href="#newer-unsupervised-ml" title="Link to this heading">#</a></h2>
<section id="embeddings">
<h3>Embeddings<a class="headerlink" href="#embeddings" title="Link to this heading">#</a></h3>
<p>When we talk about making <strong>embeddings</strong>, we’re talking about representing our data points or objects in our dataset as meaningful vectors. In graphs, and in natural language, we <em>could</em> represent nodes and words as sparse vectors. These might look like the sparse vectors we saw in the previous crappy example, where we ran k-means clustering on the adjacency matrix, or they might look like one-hot representations whose length is equal to the cardinality of a language’s entire vocabulary. Of course, representations like these aren’t very useful when we think about looking at questions of similarity or really any sort of machine learning applications.</p>
<p>Enter <strong>embeddings</strong>. Embeddings can result from dimensionality reduction techniques like principal component analysis (PCA), but nowadays, when we think about embeddings, we tend to think about vectors that are created via training a neural network model to do a task.</p>
</section>
<section id="neural-networks">
<h3>Neural Networks<a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h3>
<p>(this section borrows heavily from the coverage of ML in Bagrow &amp; Ahn, section 16.2)</p>
<p>These can seem complicated to understand, but they, are at their heart, just doing a bunch of very parallelizable matrix operations. Here’s a picture of a simple <strong>feed-forward</strong> neural network, sourced from this handy <a class="reference external" href="https://medium.com/analytics-vidhya/in-depth-explanation-of-feedforward-in-neural-network-mathematically-448092216b63">Medium post</a>.</p>
<p><img alt="feed-forward NN" src="https://miro.medium.com/v2/resize:fit:3200/1*ycDUAMaxDYaHx7xl9pqEjg.png" /></p>
<p>Neural networks consist of sets of units (neurons) arranged in a sequence of layers. When we talk about the <strong>architecture</strong> of a neural network, we’re talking about how the units are arranged, how many of them there are, how many layers we have, and the number of units per layer. In a fully connected feed-forward network, each unit in each layer is connected to all the nodes in the layer after it. First, the input nodes (the green ones) receive the input data (this could be attributes of a product, for example). Data moves through the network in the picture from left to right. The output of a particular unit is the activation function of the weighted sum of its inputs (<span class="math notranslate nohighlight">\(\sigma(\vec{w}^T \vec{x})\)</span>), where <span class="math notranslate nohighlight">\(\vec{w}\)</span> is the weights of the inputs for that unit, and <span class="math notranslate nohighlight">\(\vec{x}\)</span> is the inputs coming from the previous layer. <span class="math notranslate nohighlight">\(\sigma\)</span> is the nonlinear activation function; in this case, we’re probably thinking of the sigmoid function (pictured below &amp; sourced <a class="reference external" href="https://groups.csail.mit.edu/medg/hamish/medcomp3/sld007.htm">here</a>), but other activation functions, like <span class="math notranslate nohighlight">\(\tanh\)</span>, are possible as well (image sourced <a class="reference external" href="https://sebastianraschka.com/faq/docs/activation-functions.html">here</a>.</p>
<p><img alt="sigmoid" src="https://groups.csail.mit.edu/medg/hamish/medcomp3/img007.gif" /></p>
<p><img alt="activations" src="https://sebastianraschka.com/images/faq/activation-functions/activation-functions.png" /></p>
<p>The outputs we get from each unit in the middle layer then are passed to the final layer, and then we compare the final layer’s output to our desired output. Maybe the <span class="math notranslate nohighlight">\(\vec{y}\)</span> outputs in the picture are probabilities of the product belonging to a particular class, for example. Our task is to use a <strong>loss function</strong>, of which there are many, to express how far off we are from our correct answer. Once we’ve done that, we can use an algorithm called <strong>backpropagation</strong> to nudge the weights (<span class="math notranslate nohighlight">\(\vec{w}\)</span>) towards values that would be closer to producing the desired label.</p>
<p>Our choice of loss function is important here; what works well for classifying products may be terrible for predicting flight prices, for example. For a more complete primer on loss functions, check out <a class="reference external" href="https://www.geeksforgeeks.org/loss-functions-in-deep-learning/">this blog post</a>. Generally, your loss function and the tasks you’re setting up the neural network to learn will reflect what you want your neural network to do effectively. We might use a squared error loss function if we’re predicting flight prices, and we might use a cross-entropy loss function to quantify how incorrect our predictions are for a classifier. There are more complicated loss functions and task setups, one of which we’ll discuss in the next section.</p>
</section>
<section id="tovec">
<h3>*tovec<a class="headerlink" href="#tovec" title="Link to this heading">#</a></h3>
<section id="word2vec">
<h4>Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://arxiv.org/pdf/1301.3781">Word2Vec</a> was a pretty early natural language processing technique that was very exciting at the time. It trained a fairly simple neural network to do one of two tasks: Continuous Bag-of-Words (CBOW) or Skip-Gram. CBOW involves predicting which word is missing from a given context of surrounding words, and Skip-Gram involves doing the opposite: predicting the surrounding context given a specific word. The loss function here is cross-entropy loss; we use a softmax activation function to turn the outputs of the final layer into a probability distribution and then use cross-entropy loss to quantify how incorrect we were. Here’s some more info; the links are where we sourced our images for <a class="reference external" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/">softmax</a> and <a class="reference external" href="https://devopedia.org/word2vec">word2vec</a>.</p>
<p><img alt="word2vec tasks" src="https://devopedia.org/images/article/221/9279.1570465016.png" /></p>
<p><img alt="softmax + cross-entropy" src="https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png" /></p>
</section>
<section id="node2vec">
<h4>Node2Vec<a class="headerlink" href="#node2vec" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://arxiv.org/pdf/1607.00653">Node2Vec</a> follows similar logic to Word2Vec. We do random walks through a network and treat these random walks like sentences. Then we can do CBOW or Skip-Gram to obtain node embeddings. At each iteration, Node2Vec samples <span class="math notranslate nohighlight">\(n * r\)</span> random walks (with <span class="math notranslate nohighlight">\(r\)</span> walks starting at each node). It has hyperparameters that influence how much we revisit nodes (<span class="math notranslate nohighlight">\(p\)</span>) and how much we favor visiting close nodes versus nodes that are further from where we started (<span class="math notranslate nohighlight">\(q\)</span>).</p>
<p>When people around the time of Node2Vec were thinking about building node embeddings, they tended to consider homophily (nodes that are linked are likely to be similar) or structural equivalence (nodes with similarly structured neighborhoods are likely to be similar). Random walks that are closer to depth-first search tend to get further away from their start node, so they’ll have a better sense of the community structure of a given graph and we’ll get more information about node homophily (in theory). However, longer-range dependencies can be harder to make sense of and more complicated. If we instead do something more like a breadth-first search, we’re thinking more about structural equivalence – whose neighborhoods looks similar? This isn’t likely to explore more distant parts of the graph, and nodes will repeat themselves a lot in a given walk. So Grover &amp; Leskovec structured their random walks with hyperparameters <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> that allow us to figure out where on the BFS - DFS spectrum we want to end up.</p>
<p>Our non-normalized probability <span class="math notranslate nohighlight">\(\alpha(v, x) * w(v, x)\)</span> of visiting a given node <span class="math notranslate nohighlight">\(x\)</span> after having visited <span class="math notranslate nohighlight">\(v\)</span> in the previous step is:
$<span class="math notranslate nohighlight">\(
\alpha(v, x) = 
\begin{cases}
\frac{1}{p} &amp; if d(v, x) = 0 \\
1 &amp; if d(v, x) = 1 \\
\frac{1}{q} &amp; if d(v, x) = 2
\end{cases}
\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(d(i, j)\)</span> is the number of hops between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>Here’s an image from the original Node2Vec paper of the different outcomes of Node2Vec on a network generated from the novel <em>Les Misérables</em> with hyperparameter settings that lean more towards prioritizing homophily (closer to DFS) and structural equivalence (closer to BFS).</p>
<p><img alt="lesmis" src="../../_images/les_mis_node2vec.png" /></p>
<p>Node2Vec also incorporates <strong>negative sampling</strong>, where we only look at the outputs for a couple nodes that definitely <strong>shouldn’t</strong> be in an input node’s neighborhood. We make sure our outputs for those nodes are zero and do backpropagation accordingly; it’s possible to tune how many negative samples we do for each positive sample. Adding negative samples improves our model’s performance, and it’s relatively cheap to do because we’re not tuning all the weights in the model at once.</p>
<p>We can do a version of Node2Vec where we’re making embeddings for embeddings’ sake, as we do in this setting, or we can try to train a classifier on Node2Vec embeddings to predict labels for nodes. Here we’re going to train Node2Vec (without node or edge labels) on the Cambridge, MA network data. Let’s see what happens! Our training code is borrowed from <code class="docutils literal notranslate"><span class="pre">pytorch-geometric</span></code> and can be found <a class="reference external" href="https://github.com/pyg-team/pytorch_geometric/blob/master/examples/node2vec.py">here</a>.</p>
</section>
</section>
</section>
<section id="on-training-neural-networks">
<h2>On Training Neural Networks<a class="headerlink" href="#on-training-neural-networks" title="Link to this heading">#</a></h2>
<p>We’re going to use <a class="reference external" href="https://pytorch.org/"><code class="docutils literal notranslate"><span class="pre">pytorch</span></code></a> and <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/"><code class="docutils literal notranslate"><span class="pre">pytorch-geometric</span></code></a> to make our neural network construction relatively pain-free. Pytorch is a widely used package for building &amp; training neural networks. It’s compatible with CUDA-capable GPUs (CUDA is Nvidia’s deep learning framework that makes doing ML in GPUs possible).</p>
<section id="gpus">
<h3>GPUs<a class="headerlink" href="#gpus" title="Link to this heading">#</a></h3>
<p>GPUs, or <em>graphical processing units</em>, are great at doing a lot of tiny calculations at the same time. This is different from CPUs, which are good at doing a few big calculations at once (this is a vast oversimplification, but it’s good enough for an overview). Neural networks do a lot of vector and matrix multiplication (Remember those dot products from before? How do they scale up to a whole layer?) which is very easy to parallelize on a GPU. We’re using GPUs today to speed up our Node2Vec training, which would be pretty slow on a CPU.</p>
</section>
<section id="train-validation-test-sets">
<h3>Train/Validation/Test Sets<a class="headerlink" href="#train-validation-test-sets" title="Link to this heading">#</a></h3>
<p>A lot of the time, when we train a model (this goes for all models, not just neural network models!) we’re worried about <strong>overfitting</strong>. Overfitting happens when your model learns the data it’s trained on too well – it’s too accustomed to the training data and doesn’t generalize well to new data. We might have too many parameters in our model, such that it’s able to memorize the quirks of the training set too well, or we might’ve selected our training set improperly.</p>
<p>I once saw an example from a talk about explainability where scientists trained a model to guess where a Google StreetView photo was taken; it did really well on images taken at the same time as other images in the training set and really poorly on images taken at a later date. Turns out that the model was memorizing how windshield smudges changed over time and correlated the state of the windshield smudges to the car’s location. So when the smudges changed (but the landscape didn’t), the model was not able to identify the locations anymore.</p>
<p>We usually split our data into 3 sets – training, validation, and test. We use our <strong>training</strong> set to train the model; the <strong>validation</strong> set is used to figure out which choice of hyperparameters (like learning rate, optimizer, network size, etc.) works best for the dataset &amp; model; and the <strong>testing</strong> set is for figuring out how well the finalized model does. The model <strong>never</strong> trains on the validation or testing data - it only updates itself based on the training data. A good rule of thumb is to use 10-15% of data for validation, 15-20% of the data for testing, and the rest for training. You should randomly split up your data such that the training, testing, and validation sets have similar distributions.</p>
<p>Here, though, we’re just trying to build embeddings to demonstrate Node2Vec’s capabilities, so we’re not holding out any data right now. Node2Vec <em>does</em> sample negative examples as it trains, so that the model learns which contexts or nodes are correct and builds accurate embeddings. If we were training a link prediction model with Node2Vec, as you can choose to do at the end of this class, we would at minimum hold out some fraction of links to make sure our embeddings were of good quality and useful for the downstream link prediction task.</p>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Link to this heading">#</a></h3>
<p><strong>Hyperparameters</strong> are parameters that control how we find the neural network’s weights (the non-hyper parameters, or just plain parameters). Examples of hyperparameters are learning rate, number of layers, choice of optimizer, layer size, and activation function(s). Finding good hyperparameters can take a lot of time, depending on the number and types of hyperparameters you’re playing with; platforms like <a class="reference external" href="https://wandb.ai/site">Weights and Biases</a> have sprung up to help developers tune hyperparameters for their neural network models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Node2Vec</span> 

<span class="c1"># giving nodes new indices, as Node2Vec expects indices to start at 0 and end at |N| - 1. </span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">):</span>
    <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">node</span><span class="p">][</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>

<span class="c1"># build a tensor with all the edges in the graph. </span>
<span class="n">my_tensor</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">:</span>
    <span class="n">my_tensor</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="s1">&#39;idx&#39;</span><span class="p">],</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="s1">&#39;idx&#39;</span><span class="p">]])</span>

<span class="c1"># actually convert it to a torch Tensor -- </span>
<span class="c1"># this has to be a torch datatype or it will not play nicely with the GPU.</span>
<span class="n">edge_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="c1"># checking if we have a CUDA-capable GPU available.</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># constructing our Node2Vec model and giving it our edge list as input. </span>
<span class="c1"># note that you can tweak all of these parameters!</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Node2Vec</span><span class="p">(</span>
    <span class="n">edge_list</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">walk_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">walks_per_node</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">num_negative_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># we have 2 CPU cores requested by default in our sessions. </span>
<span class="c1"># (we use CPU cores to load our data).</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;linux&#39;</span> <span class="k">else</span> <span class="mi">0</span>

<span class="c1"># we use batches to work through our dataset without overloading the GPU.</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loader</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
<span class="c1"># you can play with optimizer choices; Adam and AdaGrad are popular choices.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SparseAdam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">):</span> <span class="c1"># an epoch refers to going over all the data once</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># set the model to training mode; </span>
    <span class="c1"># this lets it accumulate gradients and do backpropagation</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pos_rw</span><span class="p">,</span> <span class="n">neg_rw</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># zero out the gradients to start fresh</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">pos_rw</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">neg_rw</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span> 
        <span class="c1"># compute the loss (cross-entropy)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># do backpropagation</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># updates the parameters</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># keeps track of our total loss (it should decrease over time)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span> <span class="c1"># how wrong are we on average?</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="c1"># keep track of our progress</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cuda
Epoch: 001, Loss: 2.9646
Epoch: 002, Loss: 1.5299
Epoch: 003, Loss: 1.1161
Epoch: 004, Loss: 0.9574
Epoch: 005, Loss: 0.8835
Epoch: 006, Loss: 0.8442
Epoch: 007, Loss: 0.8218
Epoch: 008, Loss: 0.8073
Epoch: 009, Loss: 0.7979
Epoch: 010, Loss: 0.7913
Epoch: 011, Loss: 0.7869
Epoch: 012, Loss: 0.7838
Epoch: 013, Loss: 0.7814
Epoch: 014, Loss: 0.7797
Epoch: 015, Loss: 0.7784
Epoch: 016, Loss: 0.7775
Epoch: 017, Loss: 0.7768
Epoch: 018, Loss: 0.7759
Epoch: 019, Loss: 0.7756
Epoch: 020, Loss: 0.7751
Epoch: 021, Loss: 0.7748
Epoch: 022, Loss: 0.7747
Epoch: 023, Loss: 0.7744
Epoch: 024, Loss: 0.7742
Epoch: 025, Loss: 0.7740
Epoch: 026, Loss: 0.7739
Epoch: 027, Loss: 0.7736
Epoch: 028, Loss: 0.7735
Epoch: 029, Loss: 0.7730
Epoch: 030, Loss: 0.7729
Epoch: 031, Loss: 0.7728
Epoch: 032, Loss: 0.7729
Epoch: 033, Loss: 0.7722
Epoch: 034, Loss: 0.7725
Epoch: 035, Loss: 0.7723
Epoch: 036, Loss: 0.7720
Epoch: 037, Loss: 0.7717
Epoch: 038, Loss: 0.7716
Epoch: 039, Loss: 0.7715
Epoch: 040, Loss: 0.7715
Epoch: 041, Loss: 0.7712
Epoch: 042, Loss: 0.7711
Epoch: 043, Loss: 0.7708
Epoch: 044, Loss: 0.7708
Epoch: 045, Loss: 0.7703
Epoch: 046, Loss: 0.7706
Epoch: 047, Loss: 0.7702
Epoch: 048, Loss: 0.7701
Epoch: 049, Loss: 0.7698
Epoch: 050, Loss: 0.7694
Epoch: 051, Loss: 0.7696
Epoch: 052, Loss: 0.7695
Epoch: 053, Loss: 0.7693
Epoch: 054, Loss: 0.7692
Epoch: 055, Loss: 0.7689
Epoch: 056, Loss: 0.7690
Epoch: 057, Loss: 0.7686
Epoch: 058, Loss: 0.7688
Epoch: 059, Loss: 0.7686
Epoch: 060, Loss: 0.7682
Epoch: 061, Loss: 0.7683
Epoch: 062, Loss: 0.7681
Epoch: 063, Loss: 0.7677
Epoch: 064, Loss: 0.7678
Epoch: 065, Loss: 0.7676
Epoch: 066, Loss: 0.7672
Epoch: 067, Loss: 0.7674
Epoch: 068, Loss: 0.7672
Epoch: 069, Loss: 0.7670
Epoch: 070, Loss: 0.7669
Epoch: 071, Loss: 0.7666
Epoch: 072, Loss: 0.7667
Epoch: 073, Loss: 0.7666
Epoch: 074, Loss: 0.7668
Epoch: 075, Loss: 0.7668
Epoch: 076, Loss: 0.7664
Epoch: 077, Loss: 0.7665
Epoch: 078, Loss: 0.7664
Epoch: 079, Loss: 0.7662
Epoch: 080, Loss: 0.7661
Epoch: 081, Loss: 0.7660
Epoch: 082, Loss: 0.7659
Epoch: 083, Loss: 0.7658
Epoch: 084, Loss: 0.7660
Epoch: 085, Loss: 0.7654
Epoch: 086, Loss: 0.7659
Epoch: 087, Loss: 0.7659
Epoch: 088, Loss: 0.7656
Epoch: 089, Loss: 0.7655
Epoch: 090, Loss: 0.7655
Epoch: 091, Loss: 0.7657
Epoch: 092, Loss: 0.7655
Epoch: 093, Loss: 0.7655
Epoch: 094, Loss: 0.7654
Epoch: 095, Loss: 0.7652
Epoch: 096, Loss: 0.7654
Epoch: 097, Loss: 0.7650
Epoch: 098, Loss: 0.7656
Epoch: 099, Loss: 0.7654
Epoch: 100, Loss: 0.7652
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> 
<span class="c1"># set model to eval mode - this means it doesn&#39;t accumulate gradients</span>
<span class="c1"># and uses way less memory</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span> <span class="c1"># get embeddings</span>
<span class="n">z_cpu</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> 
<span class="c1"># put these on the CPU and detach it from the neural network. </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">contextily</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cx</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">adj_inv_length_kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">z_cpu</span><span class="p">)</span>
<span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_adj_inv_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">rgb2hex</span><span class="p">(</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adj_inv_length_kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">geo_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">markersize</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">geo_df</span><span class="p">[</span><span class="s2">&quot;label_adj_inv_length&quot;</span><span class="p">],</span> 
<span class="p">)</span>
<span class="n">cx</span><span class="o">.</span><span class="n">add_basemap</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">crs</span><span class="o">=</span><span class="n">geo_df</span><span class="o">.</span><span class="n">crs</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6780071b61144cc0884ef11bb9d2a30a755b417c07ce9a855150dc24b6a12477.png" src="../../_images/6780071b61144cc0884ef11bb9d2a30a755b417c07ce9a855150dc24b6a12477.png" />
</div>
</div>
</section>
</section>
<section id="your-turn-choose-your-own-adventure">
<h2>Your Turn: Choose Your Own Adventure!<a class="headerlink" href="#your-turn-choose-your-own-adventure" title="Link to this heading">#</a></h2>
<p>We’re now going to spend some time playing with Node2Vec. There are two tasks you can try tackling; the first one has to do with perturbation sensitivity, and the second one involves link prediction.</p>
<section id="perturbation-sensitivity">
<h3>Perturbation Sensitivity<a class="headerlink" href="#perturbation-sensitivity" title="Link to this heading">#</a></h3>
<p>If you’re interested in exploring the limits of Node2Vec qualitatively and playing around with the network, this is the right task for you! Try randomly removing some percentage of edges from the original network, and then redo the embeddings. What does the new map look like? At what point do the embeddings start to degrade, and how do you know they’ve degraded? Feel free to try this experiment on an area you know well so that you can apply your intuition more easily. You can also try adding random edges to the network instead of removing random edges and repeat this experiment.</p>
</section>
<section id="link-prediction">
<h3>Link Prediction<a class="headerlink" href="#link-prediction" title="Link to this heading">#</a></h3>
<p>If you’re interested in trying out a bit of link prediction and playing with Node2Vec’s parameters, this challenge is for you! Try randomly removing some percentage of edges from the original network, and make the embeddings. Keep track of the edges you’ve removed; these will be your positive examples. Then, create about the same number of negative examples (edges that did not appear in the original network). This will be your evaluation set. Using this evaluation set, build a classifier (a <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">random forest</a> might be a good start) that takes two nodes’ embeddings from Node2Vec as input (i.e. concatenates the embeddings for both endpoints of an edge) and learns to predict whether the edges are in the graph. Run <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html">cross-validation</a> with this classifier and let us know how it does. You can try varying hyperparameters like the learning rate or the percent of edges you remove.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your Turn!</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">SPLIT</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="c1"># fraction of held-out (removed) edges</span>

<span class="n">valid_edges</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">my_tensor_split</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">held_out_edges</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">:</span>
    <span class="n">idx0</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span>
    <span class="n">idx1</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span>
    <span class="n">valid_edges</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">idx0</span><span class="p">,</span> <span class="n">idx1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">SPLIT</span><span class="p">:</span>
        <span class="n">my_tensor_split</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">idx0</span><span class="p">,</span> <span class="n">idx1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">held_out_edges</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">idx0</span><span class="p">,</span> <span class="n">idx1</span><span class="p">])</span>
        
<span class="n">edge_list_split</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">my_tensor_split</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Node2Vec</span><span class="p">(</span>
    <span class="n">edge_list_split</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">walk_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">walks_per_node</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">num_negative_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;linux&#39;</span> <span class="k">else</span> <span class="mi">0</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loader</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SparseAdam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pos_rw</span><span class="p">,</span> <span class="n">neg_rw</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">pos_rw</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">neg_rw</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">z_split</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="n">z_cpu_split</span> <span class="o">=</span> <span class="n">z_split</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cuda
Epoch: 001, Loss: 2.7818
Epoch: 002, Loss: 1.4846
Epoch: 003, Loss: 1.0998
Epoch: 004, Loss: 0.9482
Epoch: 005, Loss: 0.8781
Epoch: 006, Loss: 0.8401
Epoch: 007, Loss: 0.8175
Epoch: 008, Loss: 0.8029
Epoch: 009, Loss: 0.7930
Epoch: 010, Loss: 0.7861
Epoch: 011, Loss: 0.7811
Epoch: 012, Loss: 0.7774
Epoch: 013, Loss: 0.7747
Epoch: 014, Loss: 0.7726
Epoch: 015, Loss: 0.7709
Epoch: 016, Loss: 0.7697
Epoch: 017, Loss: 0.7688
Epoch: 018, Loss: 0.7680
Epoch: 019, Loss: 0.7674
Epoch: 020, Loss: 0.7668
Epoch: 021, Loss: 0.7664
Epoch: 022, Loss: 0.7661
Epoch: 023, Loss: 0.7656
Epoch: 024, Loss: 0.7653
Epoch: 025, Loss: 0.7654
Epoch: 026, Loss: 0.7652
Epoch: 027, Loss: 0.7649
Epoch: 028, Loss: 0.7649
Epoch: 029, Loss: 0.7647
Epoch: 030, Loss: 0.7645
Epoch: 031, Loss: 0.7642
Epoch: 032, Loss: 0.7642
Epoch: 033, Loss: 0.7641
Epoch: 034, Loss: 0.7637
Epoch: 035, Loss: 0.7638
Epoch: 036, Loss: 0.7637
Epoch: 037, Loss: 0.7634
Epoch: 038, Loss: 0.7634
Epoch: 039, Loss: 0.7632
Epoch: 040, Loss: 0.7630
Epoch: 041, Loss: 0.7631
Epoch: 042, Loss: 0.7629
Epoch: 043, Loss: 0.7629
Epoch: 044, Loss: 0.7625
Epoch: 045, Loss: 0.7624
Epoch: 046, Loss: 0.7621
Epoch: 047, Loss: 0.7621
Epoch: 048, Loss: 0.7620
Epoch: 049, Loss: 0.7619
Epoch: 050, Loss: 0.7616
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fake_edges</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_edges</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">held_out_edges</span><span class="p">):</span>
    <span class="n">pr</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">pr</span> <span class="ow">in</span> <span class="n">valid_edges</span> <span class="ow">or</span> <span class="n">pr</span> <span class="ow">in</span> <span class="n">fake_edges</span> <span class="ow">or</span> <span class="n">pr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pr</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fake_edges</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">pr</span><span class="p">)</span>

<span class="n">fake_edges</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">fake_edges</span><span class="p">)</span>
<span class="n">edge_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ground_truths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">held_out_edges</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_edges</span><span class="p">)</span>
<span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">held_out_edges</span> <span class="o">+</span> <span class="n">fake_edges</span><span class="p">:</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">z_cpu_split</span><span class="p">[</span><span class="n">edge</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">z_cpu_split</span><span class="p">[</span><span class="n">edge</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
    <span class="n">edge_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">edge_embeddings</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ground_truths</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.86991675, 0.94148255])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/class_14"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-of-today-s-class">Goals of today’s class:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-unsupervised-machine-learning">What is Unsupervised Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#old-unsupervised-ml">“Old” Unsupervised ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-adventure">Today’s Adventure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering">K-Means Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-based-clustering">Network-Based Clustering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering">Spectral Clustering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-we-doing-it-s-dimensionality-reduction">What Are We Doing? It’s Dimensionality Reduction!</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#newer-unsupervised-ml">Newer Unsupervised ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings">Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tovec">*tovec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#node2vec">Node2Vec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#on-training-neural-networks">On Training Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpus">GPUs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-validation-test-sets">Train/Validation/Test Sets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn-choose-your-own-adventure">Your Turn: Choose Your Own Adventure!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perturbation-sensitivity">Perturbation Sensitivity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#link-prediction">Link Prediction</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Brennan Klein
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>