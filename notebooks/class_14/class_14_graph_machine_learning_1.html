
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Class 14: Machine Learning 1 - General &#8212; PHYS 7332: Network Science Data &amp; Models (Fall 2025)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/class_14/class_14_graph_machine_learning_1';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Class 15: Graph Machine Learning 2" href="../class_15/class_15_graph_ml_2.html" />
    <link rel="prev" title="Class 13: Visualization 2 — Advanced Python" href="../class_13/class_13_visualization_python.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
  
    <p class="title logo__title">PHYS 7332: Network Science Data & Models (Fall 2025)</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    PHYS 7332: Network Science Data & Models – Fall 2025
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../class_00/class_00_github_computing_setup.html">Class 00: Introduction and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_01/class_01_python_refresher.html">Class 01: Python Refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_02/class_02_networkx1.html">Class 02: Introduction to Networkx 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_03/class_03_networkx2.html">Class 03: Introduction to Networkx 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_04/class_04_distributions.html">Class 04: Distributions of Network Properties &amp; Centralities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_05/class_05_scraping1.html">Class 05: Scraping Web Data 1 - BeautifulSoup &amp; HTML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_06/class_06_data_science_and_sql.html">Class 06: Data Science 1 — Pandas, SQL, Regressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_07/class_07_creating_a_network_from_sql_tables.html">Class 07: Data Science 2 — SQL for Network Construction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_08/class_08_communities1.html">Class 08: Clustering &amp; Community Detection 1 — Traditional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_09/class_09_communities2.html">Class 09: Clustering &amp; Community Detection 2 — Contemporary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_12/class_12_visualization_python.html">Class 12: Visualization 1 — Python and Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_13/class_13_visualization_python.html">Class 13: Visualization 2 — Advanced Python</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Class 14: Machine Learning 1 - General</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_15/class_15_graph_ml_2.html">Class 15: Machine Learning 2 - Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_16/class_16_dynamics1.html">Chapter 16: Dynamics on Networks 1 — Diffusion and Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../class_17/class_17_dynamics2.html">Chapter 17: Dynamics on Networks 2 — Compartmental Models</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/network-science-data-and-models/phys7332_fa25" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/network-science-data-and-models/phys7332_fa25/issues/new?title=Issue%20on%20page%20%2Fnotebooks/class_14/class_14_graph_machine_learning_1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/class_14/class_14_graph_machine_learning_1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Class 14: Machine Learning 1 - General</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-dataset">Today’s Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-ml-principles">Basic ML Principles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-splits">Train/Test Splits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-how-well-we-do">Measuring How Well We Do</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collective-classification">Collective Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-classification">Iterative Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">Your Turn!</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#link-prediction">Link Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-based-methods">Similarity-Based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-limited-whirlwind-tour-of-similarity-measures">A Limited Whirlwind Tour of Similarity Measures</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#for-discussion">For Discussion:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifier-based-methods">Classifier-Based Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#freeform-experimentation">Freeform Experimentation!</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-time">Next time…</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-resources">References and further resources:</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="class-14-machine-learning-1-general">
<h1>Class 14: Machine Learning 1 - General<a class="headerlink" href="#class-14-machine-learning-1-general" title="Link to this heading">#</a></h1>
<p>Goals of today’s class:</p>
<ol class="arabic simple">
<li><p>Understand, at a high level, how to train and evaluate a classifier.</p></li>
<li><p>Implement some classifiers (that don’t involve neural networks) on a network dataset.</p></li>
<li><p>Experiment with tweaking classifier type, hyperparameters, and training data size. Reason about how this impacts performance.</p></li>
</ol>
<hr class="docutils" />
<ol class="arabic simple">
<li><p>Come in. Sit down. Open Teams.</p></li>
<li><p>Find your notebook in your /Class_14/ folder.</p></li>
</ol>
<hr class="docutils" />
<section id="today-s-dataset">
<h2>Today’s Dataset<a class="headerlink" href="#today-s-dataset" title="Link to this heading">#</a></h2>
<p>The network you’re about to look at is a German language network of Twitch streamers. Each node represents a user, and an edge between them means that they have a mutual friendship (so this is an undirected network). We also have some limited data about each user’s account. There’s a binary classification task for this dataset that involves predicting whether or not a streamer uses explicit language.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">networkx</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nx</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">read_edgelist</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./data/twitch/DE/musae_DE_edges.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">nodetype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># edges between users</span>
<span class="c1"># has info about each account plus binary indicators of whether a streamer uses explicit language</span>
<span class="n">df_attrs_explicit</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./data/twitch/DE/musae_DE_target.csv&#39;</span><span class="p">)</span> 
<span class="n">attrs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">a</span><span class="p">[</span><span class="s1">&#39;new_id&#39;</span><span class="p">]:</span> <span class="p">{</span>
        <span class="s1">&#39;days&#39;</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="s1">&#39;days&#39;</span><span class="p">],</span> 
        <span class="s1">&#39;views&#39;</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="s1">&#39;views&#39;</span><span class="p">],</span> 
        <span class="s1">&#39;partner&#39;</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="s1">&#39;partner&#39;</span><span class="p">]</span>
    <span class="p">}</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">df_attrs_explicit</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">nx</span><span class="o">.</span><span class="n">set_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ml_features&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="basic-ml-principles">
<h2>Basic ML Principles<a class="headerlink" href="#basic-ml-principles" title="Link to this heading">#</a></h2>
<p>Machine learning basically involves building algorithms that can learn things from data and make generalizations to new data.
Supervised machine learning is the practice of building a model when we have training data – the information that we use to inform our model how the world works – that has correct ground truth answers. When you’re building a linear regression or a Naive Bayes classifier (for example), you’re doing supervised machine learning.</p>
<p>In contrast, unsupervised machine learning (which we won’t be doing much of today) is useful when we don’t have access to exact ground truths about desired outcomes, but we have to build insights with our dataset anyway. When we do unsupervised ML, we figure out what patterns are present in unlabeled data. Clustering is one type of unsupervised machine learning technique; when we cluster a dataset, we’re learning to recognize similar kinds of data points. Generating new examples is also (kind of) an unsupervised ML task. Training GPT and its ilk to produce language requires massive amounts of scraped text that doesn’t have “answers” attached to it per se. Humans aren’t going through and labeling the terabytes of data that GPT is trained on (before it is fine-tuned for specific tasks). Instead, at a high level, GPT and friends are trained to produce text that we can’t tell comes from a computer – it should “look like” the text that showed up in the massive amounts of data it was trained on. Once GPT has been trained in this fashion, it is also fine-tuned for specific tasks – this is where human labeling and direction do come in.</p>
<p>Today, when we talk about doing machine learning we’re going to mostly focus on supervised ML, specifically <em>binary classification</em>. Binary classification involves learning to answer a yes-or-no question about new data points after optimizing a model’s parameters using a set of data points with established answers.</p>
<section id="train-test-splits">
<h3>Train/Test Splits<a class="headerlink" href="#train-test-splits" title="Link to this heading">#</a></h3>
<p>Training models requires training data and test data at minimum. Our training data is what the model learns patterns from, and the test data tells us how well the model does on data it hasn’t seen before.</p>
<p><strong>If we were to test on the same data we trained on, what could potentially happen?</strong></p>
<hr class="docutils" />
<p>Spoiler: This is called <strong>overfitting</strong>. A lot of the time, when we train a model we’re worried about this happening. Overfitting happens when your model learns the data it’s trained on too well – it’s too accustomed to the training data and doesn’t generalize well to new data. We might have too many parameters in our model, such that it’s able to memorize the quirks of the training set too well, or we might’ve selected our training set improperly.</p>
<p>I once saw an example from a talk about explainability where scientists trained a model to guess where a Google StreetView photo was taken; it did really well on images taken at the same time as other images in the training set and really poorly on images taken at a later date. Turns out that the model was memorizing how windshield smudges changed over time and correlated the state of the windshield smudges to the car’s location. So when the smudges changed (but the landscape didn’t), the model was not able to identify the locations anymore.</p>
<p>When we construct training and testing sets, we need to make sure the data that we use for training has similar patterns to the test data while keeping the training and testing sets strictly separate from each other. We usually accomplish this by randomly splitting the dataset into two distinct parts. In this case, we’ll be using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, also known as <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, which is a general-purpose machine learning library for Python. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> has a handy function called <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> that we can use to generate a training set and test set that are randomly selected. We can also set the <code class="docutils literal notranslate"><span class="pre">random_seed</span></code> parameter to be a fixed number; this will ensure that we get the same train/test split every time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">df_attrs_explicit</span> <span class="o">=</span> <span class="n">df_attrs_explicit</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">explicit_train</span><span class="p">,</span> <span class="n">explicit_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df_attrs_explicit</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The training set length is&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explicit_train</span><span class="p">),</span> <span class="s1">&#39;and the test set length is&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explicit_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The training set length is 6648 and the test set length is 2850
</pre></div>
</div>
</div>
</div>
</section>
<section id="measuring-how-well-we-do">
<h3>Measuring How Well We Do<a class="headerlink" href="#measuring-how-well-we-do" title="Link to this heading">#</a></h3>
<p>The next thing we have to think about, once we’ve trained our model, is evaluating how well it performs on held-out data (i.e. the test set). For a binary classifier, we can define four kinds of events that might occur:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>True Value: 1</p></th>
<th class="head"><p>True Value: 0</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Predicted Value: 1</p></td>
<td><p>True Positive (TP)</p></td>
<td><p>False Positive (FP)</p></td>
</tr>
<tr class="row-odd"><td><p>Predicted Value: 0</p></td>
<td><p>False Negative (FN)</p></td>
<td><p>True Negative (TN)</p></td>
</tr>
</tbody>
</table>
</div>
<p>There are a few ways we can measure how well a binary classifier does, and sometimes we need to look at several measures in order to get a full picture of itss performance.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Measure</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Accuracy</p></td>
<td><p>How often do we get it right?</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{TP + TN}{TP + FP + TN + FN}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Recall</p></td>
<td><p>How many true positives do we detect out of all positive examples?</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{TP}{TP + FN}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Precision</p></td>
<td><p>How many of our positive classifications are truly positive?</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{TP}{TP + FP}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>F1 Score</p></td>
<td><p>Harmonic mean of precision and recall</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{2TP}{2TP + FP + FN}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>Let’s try a toy example using the dataframe with user attributes and explicit language usage. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> has a lot of classifiers; right now we’re going to try using a <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">random forest classifier</a>. Random forest classifiers consist of many <a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree classifiers</a>, each of which is trained on a different subset of the training dataset. Broadly speaking, decision tree classifiers learn decision rules from the data; they can be very complicated, but you can also force them to limit their breadth or depth. The average prediction output by all the decision trees for a particular data point is the random forest classifier’s final output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">TRAIN_COLS</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="s1">&#39;days&#39;</span><span class="p">,</span> <span class="s1">&#39;views&#39;</span><span class="p">,</span> <span class="s1">&#39;partner&#39;</span><span class="p">])</span>
<span class="n">explicit_train_x</span> <span class="o">=</span> <span class="n">explicit_train</span><span class="p">[[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">explicit_train</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">TRAIN_COLS</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">explicit_test_x</span> <span class="o">=</span> <span class="n">explicit_test</span><span class="p">[[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">explicit_test</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">TRAIN_COLS</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">explicit_train_y</span> <span class="o">=</span> <span class="n">explicit_train</span><span class="p">[</span><span class="s1">&#39;mature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">explicit_test_y</span> <span class="o">=</span> <span class="n">explicit_test</span><span class="p">[</span><span class="s1">&#39;mature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;log_loss&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">explicit_train_x</span><span class="p">,</span> <span class="n">explicit_train_y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">explicit_test_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;accuracy score:&#39;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">explicit_test_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;precision:&#39;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">explicit_test_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;recall:&#39;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">explicit_test_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;F1:&#39;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">explicit_test_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">explicit_test_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rate of True Negatives Correctly Identified:&#39;</span><span class="p">,</span> <span class="n">tn</span> <span class="o">/</span> <span class="p">(</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;False Positives Out of All Negative Labels (false alarm rate):&#39;</span><span class="p">,</span> <span class="n">fp</span> <span class="o">/</span> <span class="p">(</span><span class="n">fp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;False Negatives Out of All Positive Labels (failure to detect):&#39;</span><span class="p">,</span> <span class="n">fn</span> <span class="o">/</span> <span class="p">(</span><span class="n">fn</span> <span class="o">+</span> <span class="n">tp</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>accuracy score: 0.5835087719298245
precision: 0.6476906552094522
recall: 0.694300518134715
F1: 0.6701861628230064

Rate of True Negatives Correctly Identified: 0.4106019766397125
False Positives Out of All Negative Labels (false alarm rate): 0.5893980233602875
False Negatives Out of All Positive Labels (failure to detect): 0.30569948186528495
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Link to this heading">#</a></h3>
<p>Let’s see if we can tweak our decision tree classifier a bit. When we talk about <strong>hyperparameters</strong> in machine learning, we’re referring to the parameters that govern a model’s behavior, not the parameters that the model learns itself in order to fit the data. For a decision tree, we might control its depth or the maximum number of leaf nodes it’s allowed to make. When we are working with a classifier to figure out the best hyperparameters, we still don’t want to use our test set to see how well we’re doing when we change hyperparameters. Instead, we’ll add another split to the dataset, known as the <strong>validation set</strong>. We use the validation set to see how well we perform on data we didn’t see in training; this lets us tune our hyperparameters effectively before we see how well our final model performs on the test set.</p>
<p>When we’re tweaking a model’s hyperparameters, we usually split our data into 3 sets – training, validation, and test. We use our <strong>training</strong> set to train the model; the <strong>validation</strong> set is used to figure out which choice of hyperparameters (like learning rate, optimizer, network size, etc.) works best for the dataset &amp; model; and the <strong>testing</strong> set is for figuring out how well the finalized model does. The model <strong>never</strong> trains on the validation or testing data - it only updates itself based on the training data. A good rule of thumb is to use 10-15% of data for validation, 15-20% of the data for testing, and the rest for training. You should randomly split up your data such that the training, testing, and validation sets have similar distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="n">explicit_train</span><span class="p">,</span> <span class="n">explicit_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df_attrs_explicit</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">explicit_train_x</span> <span class="o">=</span> <span class="n">explicit_train</span><span class="p">[[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">explicit_train</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">TRAIN_COLS</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">explicit_val_x</span> <span class="o">=</span> <span class="n">explicit_val</span><span class="p">[[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">explicit_val</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">TRAIN_COLS</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">explicit_train_y</span> <span class="o">=</span> <span class="n">explicit_train</span><span class="p">[</span><span class="s1">&#39;mature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">explicit_val_y</span> <span class="o">=</span> <span class="n">explicit_val</span><span class="p">[</span><span class="s1">&#39;mature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">f1s</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">false_alarms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">failed_detections</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">explicit_train_x</span><span class="p">,</span> <span class="n">explicit_train_y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">explicit_val_x</span><span class="p">)</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">explicit_val_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
    <span class="n">f1s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">explicit_val_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
    <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">explicit_val_y</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">false_alarms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fp</span> <span class="o">/</span> <span class="p">(</span><span class="n">fp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">))</span>
    <span class="n">failed_detections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span> <span class="o">/</span> <span class="p">(</span><span class="n">fn</span> <span class="o">+</span> <span class="n">tp</span><span class="p">))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">)],</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Accuracy (higher better)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">)],</span> <span class="n">f1s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;F1 Score (higher better)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">)],</span> <span class="n">false_alarms</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;magenta&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;False Positive Rate (lower better)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">)],</span> <span class="n">failed_detections</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lime&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;False Negative Rate (lower better)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">explicit_val_y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">explicit_val_y</span><span class="p">)],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Tree Depth Vs Performance Metrics&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Maximum Decision Tree Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Score&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Score&#39;)
</pre></div>
</div>
<img alt="../../_images/238d6d5193c5557fab03d856c323697ad654ea98becfb20366dba0f6629b7d4a.png" src="../../_images/238d6d5193c5557fab03d856c323697ad654ea98becfb20366dba0f6629b7d4a.png" />
</div>
</div>
<p>As you can see, we are doing only marginally better than random guessing right now.</p>
<p><strong>What does that mean about how useful our classifier is?</strong></p>
<p>Let’s see what happens when we incorporate information about the network into our classification.</p>
</section>
</section>
<section id="collective-classification">
<h2>Collective Classification<a class="headerlink" href="#collective-classification" title="Link to this heading">#</a></h2>
<p>When we do collective classification, we have two variables: a graph <span class="math notranslate nohighlight">\(G\)</span> and a node <span class="math notranslate nohighlight">\(v \in G\)</span> that we want to label (classify). Some of <span class="math notranslate nohighlight">\(v\)</span>’s neighbors are labeled, and some are not. In this setting, we can classify <span class="math notranslate nohighlight">\(v\)</span> based on any subset of the following:</p>
<ol class="arabic simple">
<li><p>The correlation between <span class="math notranslate nohighlight">\(v\)</span>’s label and its observed attributes (this is what we just tried!)</p></li>
<li><p>The correlation between <span class="math notranslate nohighlight">\(v\)</span>’s label and its neighbors’ known labels and observed attributes</p></li>
<li><p>The correlation between <span class="math notranslate nohighlight">\(v\)</span>’s label and its unlabeled neighbors’ unknown labels.</p></li>
</ol>
<p>A lot of the collective classification methods discussed <a class="reference external" href="https://onlinelibrary.wiley.com/doi/10.1609/">here</a> (paper coauthored by our very own Dr. Tina Eliassi-Rad) involve horrifying amounts of statistical inference, which is beyond the scope of this class. However, one of the methods discussed there, <em>iterative classification</em>, is fairly tractable. Let’s take a look!</p>
<section id="iterative-classification">
<h3>Iterative Classification<a class="headerlink" href="#iterative-classification" title="Link to this heading">#</a></h3>
<p>First, you choose a local classifier, <span class="math notranslate nohighlight">\(f\)</span>, that can be used to determine the label of a node given the labels (seen &amp; unseen) of its neighbors. Tina calls this “guilt by association”. Your local classifier <span class="math notranslate nohighlight">\(f\)</span> can give you a label outright, or it can give you a probability distribution over labels (in which case you pick the label that gives you the best value). Since real-world networks notably do not have fixed degree, but most classifiers assume fixed-length feature vectors, we can’t just feed an arbitrary-length vector into any function <span class="math notranslate nohighlight">\(f\)</span>. Instead, we have to come up with some sort of pooling function, like MAX, COUNT, MEAN, SUM, etc. and pool the labels in some pre-determined way.</p>
<p>We then move on to the actual labeling business. First, we do a “bootstrapping” step – we label each node using our incomplete information, giving it our best guess. Next, we do the iterative classification process. Each time, we generate a new ordering
O of nodes, then we go through and label each node according to what its neighbors are currently labeled as, using <span class="math notranslate nohighlight">\(f\)</span>. We do this a bunch of times until the labels stop changing or we’ve gone through a pre-determined number of iterations, after which we should give up.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">explicit_train_ic</span><span class="p">,</span> <span class="n">explicit_test_ic</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df_attrs_explicit</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">map_train</span> <span class="o">=</span> <span class="p">{</span><span class="n">user</span><span class="p">:</span> <span class="n">mature</span> <span class="k">for</span> <span class="n">user</span><span class="p">,</span> <span class="n">mature</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">explicit_train_ic</span><span class="p">[</span><span class="s1">&#39;new_id&#39;</span><span class="p">],</span> <span class="n">explicit_train_ic</span><span class="p">[</span><span class="s1">&#39;mature&#39;</span><span class="p">])}</span>
<span class="n">map_test</span> <span class="o">=</span> <span class="p">{</span><span class="n">user</span><span class="p">:</span> <span class="n">mature</span> <span class="k">for</span> <span class="n">user</span><span class="p">,</span> <span class="n">mature</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">explicit_test_ic</span><span class="p">[</span><span class="s1">&#39;new_id&#39;</span><span class="p">],</span> <span class="n">explicit_test_ic</span><span class="p">[</span><span class="s1">&#39;mature&#39;</span><span class="p">])}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">local_classifier</span><span class="p">(</span><span class="n">neighbor_labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a list of neighbor labels, return True if there are at least 4 True values in the list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">neighbor_labels</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">4</span>

<span class="k">def</span><span class="w"> </span><span class="nf">iterative_classification</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">map_train</span><span class="p">,</span> <span class="n">map_test</span><span class="p">,</span> <span class="n">local_classifier</span><span class="o">=</span><span class="n">local_classifier</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform the iterative classification algorithm on the network G given split train and test sets.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    G: networkx graph object; should have the same nodes as the keys in map_train and map_test combined.</span>
<span class="sd">    map_train and map_test: dicts; mappings from the nodes in the train &amp; test sets to their binary classification ground truth.</span>
<span class="sd">    local_classifier: function that takes in a list of boolean values and returns a single boolean value</span>
<span class="sd">    max_iter: denotes how many iterations to do before stopping (if no convergence occurs).</span>

<span class="sd">    Returns:</span>
<span class="sd">    labels: mapping over the entire dataset (with labels from map_train preserved) from nodes to their binary classification.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">map_test</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">map_test</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="n">neighbor_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">map_train</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">neighbors</span> <span class="k">if</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">map_train</span><span class="p">]</span>
        <span class="n">labels</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_classifier</span><span class="p">(</span><span class="n">neighbor_labels</span><span class="p">)</span>
        
    <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">|</span> <span class="n">map_train</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">nodes_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">map_test</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
    <span class="k">while</span> <span class="n">n_iter</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="n">prev_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nodes_list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes_list</span><span class="p">:</span>
            <span class="n">neighbors</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
            <span class="n">neighbor_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">prev_labels</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_classifier</span><span class="p">(</span><span class="n">neighbor_labels</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="o">==</span> <span class="n">prev_labels</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">labels</span>
        <span class="n">n_iter</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">labels</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">iterative_classification</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">map_train</span><span class="p">,</span> <span class="n">map_test</span><span class="p">,</span> <span class="n">local_classifier</span><span class="o">=</span><span class="n">local_classifier</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">ground_truth</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">map_test</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">ground_truth</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;accuracy score:&#39;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;precision:&#39;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;recall:&#39;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;F1:&#39;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rate of True Negatives Correctly Identified:&#39;</span><span class="p">,</span> <span class="n">tn</span> <span class="o">/</span> <span class="p">(</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;False Positives Out of All Negative Labels (false alarm rate):&#39;</span><span class="p">,</span> <span class="n">fp</span> <span class="o">/</span> <span class="p">(</span><span class="n">fp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;False Negatives Out of All Positive Labels (failure to detect):&#39;</span><span class="p">,</span> <span class="n">fn</span> <span class="o">/</span> <span class="p">(</span><span class="n">fn</span> <span class="o">+</span> <span class="n">tp</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>accuracy score: 0.6390819119814698
precision: 0.6576116191500807
recall: 0.8471933471933472
F1: 0.7404603270745003

Rate of True Negatives Correctly Identified: 0.3166935050993022
False Positives Out of All Negative Labels (false alarm rate): 0.6833064949006978
False Negatives Out of All Positive Labels (failure to detect): 0.15280665280665282
</pre></div>
</div>
</div>
</div>
</section>
<section id="your-turn">
<h3>Your Turn!<a class="headerlink" href="#your-turn" title="Link to this heading">#</a></h3>
<p>Can you toy with the <code class="docutils literal notranslate"><span class="pre">local_classifier</span></code> function to produce better performance with this classification algorithm?</p>
</section>
</section>
<section id="link-prediction">
<h2>Link Prediction<a class="headerlink" href="#link-prediction" title="Link to this heading">#</a></h2>
<p>Next, we’re going to learn about predicting the existence of missing links (or links that will be formed at the next time step) in a graph, also known as <em>link prediction</em>. For a more comprehensive overview of link prediction, check out <a class="reference external" href="http://dx.doi.org/10.1145/3012704">this survey article</a> by Martinez et al., which this section draws from quite a bit. Many link prediction tasks are formulated as a ranking problem, where all pairs of nodes that aren’t connected yet get a score that is proportional to the likelihood of a link forming between them. Then we usually choose a threshold, above which we predict a link will exist. We can also view link prediction as a binary classification problem; today we will explore both points of view.</p>
<section id="similarity-based-methods">
<h3>Similarity-Based Methods<a class="headerlink" href="#similarity-based-methods" title="Link to this heading">#</a></h3>
<p>Homophily refers to the tendency of similar nodes to be connected to each other. Not all networks are completely homophilous on all dimensions (think dating networks, for example), but many are. Similarity-based methods operate on the assumption that the more similar a pair of nodes is, the more likely they are to be connected to each other. We can define similarity in many different ways, but in general, we choose a similarity measure, rank candidate node pairs by their similarity, and then pick a threshold above which we say we will predict a link exists.</p>
<section id="a-limited-whirlwind-tour-of-similarity-measures">
<h4>A Limited Whirlwind Tour of Similarity Measures<a class="headerlink" href="#a-limited-whirlwind-tour-of-similarity-measures" title="Link to this heading">#</a></h4>
<p>In this table, we’ve laid out some of the more commonly used similarity measures. Some of them are based on local structural information about a node’s neighborhood (“Local”); others use global approaches, usually involving paths between two nodes or random walks, to determine similarity (“Global”); there are also approaches that mix the two, considering more topological information than strictly local measures while still being relatively fast to compute (“Quasi-Local”. Today we’ll focus on local measures of similarity; for a node pair <span class="math notranslate nohighlight">\((u, v)\)</span>, we will define the similarity for a number of measures.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Similarity Measure</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Common Neighbors</p></td>
<td><p>How many neighbors do <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> have in common?</p></td>
<td><p>$</p></td>
</tr>
<tr class="row-odd"><td><p>Adamic-Adar Index</p></td>
<td><p>Common neighbors but normalized by neighbors’ degree</p></td>
<td><p>$\sum\limits_{w \in N(u) \cap N(v)} \frac{1}{</p></td>
</tr>
<tr class="row-even"><td><p>Preferential Attachment Index</p></td>
<td><p>Product of nodes’ number of neighbors</p></td>
<td><p>$</p></td>
</tr>
<tr class="row-odd"><td><p>Jaccard Index</p></td>
<td><p>Ratio of shared neighbors to total number of neighbors</p></td>
<td><p>$\frac{</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What are some other ways we could measure similarity/proximity in the network, perhaps looking at paths between nodes?</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="c1"># make a train/test split of edges in the graph; we&#39;ll remove 1/4 of all edges to start.</span>
<span class="n">edges</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">]</span>
<span class="n">edges_train</span><span class="p">,</span> <span class="n">edges_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># get nodes that don&#39;t show up in the training set</span>
<span class="n">nodes_in_train</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">node</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">edges_train</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">pair</span><span class="p">])</span>
<span class="n">nodes_in_test</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">node</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">edges_test</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">pair</span><span class="p">])</span>
<span class="n">node_not_in_G</span> <span class="o">=</span> <span class="n">nodes_in_test</span> <span class="o">-</span> <span class="n">nodes_in_train</span> 

<span class="c1"># construct a graph with just the known (i.e. training) edges</span>
<span class="n">G_train</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_edgelist</span><span class="p">(</span><span class="n">edges_train</span><span class="p">)</span>
<span class="c1"># make sure we have all the nodes in the original graph in case we&#39;re missing some in the set of training edges.</span>
<span class="n">G_train</span><span class="o">.</span><span class="n">add_nodes_from</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">node_not_in_G</span><span class="p">))</span> 

<span class="k">def</span><span class="w"> </span><span class="nf">generate_sample_negative_edges</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">edges_test</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="mf">2.1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a graph G and a test set of edges that will be predicted, </span>
<span class="sd">    produce a list of edges that don&#39;t exist in G OR in the test set.</span>

<span class="sd">    Parameters:</span>
<span class="sd">      G: networkx graph object; contains only edges from a training set (i.e. it&#39;s a masked version of an original graph)</span>
<span class="sd">      edges_test: the edges missing from the original graph that are not present in G</span>
<span class="sd">      multiplier: very approximately multiply the number of test edges by this to express the desired quantity of negative examples.</span>

<span class="sd">    Returns:</span>
<span class="sd">      list of candidate edges that don&#39;t exist in G OR in edges_test </span>
<span class="sd">      (negative examples for a link prediction strategy to be evaluated on).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">]</span>
    <span class="n">edge_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">])</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">edges_test</span><span class="p">))</span>
    <span class="n">combos</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">combos</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">edges_train</span><span class="p">)</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">candidates</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">edge_set</span><span class="p">]</span>

<span class="c1"># make some fake edges</span>
<span class="n">candidate_edges</span> <span class="o">=</span> <span class="n">generate_sample_negative_edges</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">edges_test</span><span class="p">)</span>
<span class="c1"># run adamic adar index on a limited subset of negative &amp; positive candidate edges</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adamic_adar_index</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges</span> <span class="o">+</span> <span class="n">edges_test</span><span class="p">)</span> 
<span class="c1"># rank edges by their similarity scores</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_max_similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">similarities</span><span class="p">])))</span> <span class="c1"># similarity is roughly distributed on a log scale</span>
<span class="n">set_edges_test</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">edges_test</span><span class="p">)</span>

<span class="c1"># evaluating precision &amp; recall at different threshold values</span>
<span class="n">precisions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">recalls</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">log_max_similarity</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">):</span>
    <span class="c1"># edges w/ similarity above the assigned threshold; contains all true &amp; false positives</span>
    <span class="n">edges_above</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">similarities</span> <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">threshold</span><span class="p">)])</span> 
    <span class="c1"># edges w/ similarity at or below the assigned threshold; contains all true &amp; false negatives</span>
    <span class="n">edges_below</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">similarities</span> <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">threshold</span><span class="p">)])</span>
    
    <span class="n">true_pos</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">set_edges_test</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">edges_above</span><span class="p">))</span> <span class="c1"># number of true positives</span>
    <span class="n">false_neg</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">set_edges_test</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">edges_below</span><span class="p">))</span> <span class="c1"># number of false negatives</span>
    <span class="n">precisions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">true_pos</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">edges_above</span><span class="p">))</span> 
    <span class="n">recalls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">true_pos</span> <span class="o">/</span> <span class="p">(</span><span class="n">true_pos</span> <span class="o">+</span> <span class="n">false_neg</span><span class="p">))</span>

<span class="c1"># plots precision and recall at various thresholds</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Link Prediction Threshold vs Precision and Recall &#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">log_max_similarity</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">precisions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;precision&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">log_max_similarity</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">recalls</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;recall&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Link Prediction Threshold (log scale)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Precision/Recall (higher is better)&#39;</span><span class="p">)</span>
                    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Precision/Recall (higher is better)&#39;)
</pre></div>
</div>
<img alt="../../_images/343b3d7d44804ebf333b0831396617f10f89abbec367ad111a4a7a10518b9b6f.png" src="../../_images/343b3d7d44804ebf333b0831396617f10f89abbec367ad111a4a7a10518b9b6f.png" />
</div>
</div>
</section>
<section id="for-discussion">
<h4>For Discussion:<a class="headerlink" href="#for-discussion" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Based on this plot, what would you choose as the approximate threshold? Why?</p></li>
<li><p>Why do you think we see the trends that we see (high threshold = terrible recall; low threshold = bad precision)?</p></li>
<li><p>Would this task get harder if we looked at all possible links (i.e. all links that didn’t exist in the training graph)?</p></li>
</ul>
</section>
</section>
<section id="classifier-based-methods">
<h3>Classifier-Based Methods<a class="headerlink" href="#classifier-based-methods" title="Link to this heading">#</a></h3>
<p>We can also use binary classifiers for link prediction. In this case, we treat our examples more independently (i.e. without ranking). We can use node features, information about network topology, and even the similarity measures we just used. Discussing the various binary classifiers that exist is beyond the scope of this lesson, but we will try out a random forest classifier on our dataset that incorporates the 4 similarity measures we just discussed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make some fake edges</span>
<span class="n">candidate_edges_train</span> <span class="o">=</span> <span class="n">generate_sample_negative_edges</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">edges_test</span><span class="p">)</span>
<span class="n">candidate_edges_test</span> <span class="o">=</span> <span class="n">generate_sample_negative_edges</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">edges_test</span> <span class="o">+</span> <span class="n">candidate_edges_train</span><span class="p">)</span>

<span class="c1"># create similarity measures for training data</span>
<span class="n">adamic_adar_train</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adamic_adar_index</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges_train</span> <span class="o">+</span> <span class="n">edges_train</span><span class="p">)</span> 
<span class="n">jaccard_train</span>  <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">jaccard_coefficient</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges_train</span> <span class="o">+</span> <span class="n">edges_train</span><span class="p">)</span>
<span class="n">pref_attachment_train</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">preferential_attachment</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges_train</span> <span class="o">+</span> <span class="n">edges_train</span><span class="p">)</span>
<span class="n">common_neighbors_train</span> <span class="o">=</span> <span class="p">[(</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">common_neighbors</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">candidate_edges_train</span> <span class="o">+</span> <span class="n">edges_train</span><span class="p">]</span>

<span class="c1"># create similarity measures for test data</span>
<span class="n">adamic_adar_test</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">adamic_adar_index</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges_test</span> <span class="o">+</span> <span class="n">edges_test</span><span class="p">)</span> 
<span class="n">jaccard_test</span>  <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">jaccard_coefficient</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges_test</span> <span class="o">+</span> <span class="n">edges_test</span><span class="p">)</span>
<span class="n">pref_attachment_test</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">preferential_attachment</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">candidate_edges_test</span> <span class="o">+</span> <span class="n">edges_test</span><span class="p">)</span>
<span class="n">common_neighbors_test</span> <span class="o">=</span> <span class="p">[(</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">common_neighbors</span><span class="p">(</span><span class="n">G_train</span><span class="p">,</span> <span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">candidate_edges_test</span> <span class="o">+</span> <span class="n">edges_test</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build numpy matrices of the similarity measures</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="n">t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">]</span> 
     <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="p">[</span><span class="n">adamic_adar_train</span><span class="p">,</span> <span class="n">jaccard_train</span><span class="p">,</span> <span class="n">pref_attachment_train</span><span class="p">,</span> <span class="n">common_neighbors_train</span><span class="p">]</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_edges_train</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">edges_train</span><span class="p">))))</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="n">t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">]</span> 
     <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="p">[</span><span class="n">adamic_adar_test</span><span class="p">,</span> <span class="n">jaccard_test</span><span class="p">,</span> <span class="n">pref_attachment_test</span><span class="p">,</span> <span class="n">common_neighbors_test</span><span class="p">]</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_edges_test</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">edges_test</span><span class="p">))))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a random forest classifier; fit it to the training data &amp; predict for the test data.</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;log_loss&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Briefly, how well did we do?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;accuracy score:&#39;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;precision:&#39;</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;recall:&#39;</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;F1:&#39;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>accuracy score: 0.9088524212089107
precision: 0.6440659170049017
recall: 0.7584954943189238
F1: 0.6966127716739433
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="freeform-experimentation">
<h2>Freeform Experimentation!<a class="headerlink" href="#freeform-experimentation" title="Link to this heading">#</a></h2>
<p>For the remainder of the class period, pick a question you’d like to answer with respect to link prediction, and write code to answer it.</p>
<p>Here are some examples:</p>
<ul class="simple">
<li><p>How does classifier performance change as the ratio of negative to positive examples increases?</p></li>
<li><p>Which kinds of classifiers in the <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html">sklearn binary classification pantheon</a> perform better for link prediction?</p></li>
<li><p>What happens if we add in new <a class="reference external" href="https://networkx.org/documentation/stable/reference/algorithms/similarity.html">similarity measures</a> to the code from the previous example? What if we remove some similarity measures?</p></li>
<li><p>What if we drastically change the prevalence of negative examples in the training set relative to the prevalence in the test set?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your Turn!</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="next-time">
<h2>Next time…<a class="headerlink" href="#next-time" title="Link to this heading">#</a></h2>
<p>Introduction to Machine Learning 2 — Graph ML <code class="docutils literal notranslate"><span class="pre">class_15_graph_ml_2</span></code></p>
</section>
<hr class="docutils" />
<section id="references-and-further-resources">
<h2>References and further resources:<a class="headerlink" href="#references-and-further-resources" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Class Webpages</p>
<ul class="simple">
<li><p>Jupyter Book: <a class="reference external" href="https://network-science-data-and-models.github.io/phys7332_fa25/README.html">https://network-science-data-and-models.github.io/phys7332_fa25/README.html</a></p></li>
<li><p>Github: <a class="github reference external" href="https://github.com/network-science-data-and-models/phys7332_fa25/">network-science-data-and-models/phys7332_fa25</a></p></li>
<li><p>Syllabus and course details: <a class="reference external" href="https://brennanklein.com/phys7332-fall25">https://brennanklein.com/phys7332-fall25</a></p></li>
</ul>
</li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/class_14"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../class_13/class_13_visualization_python.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Class 13: Visualization 2 — Advanced Python</p>
      </div>
    </a>
    <a class="right-next"
       href="../class_15/class_15_graph_ml_2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Class 15: Graph Machine Learning 2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-dataset">Today’s Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-ml-principles">Basic ML Principles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-splits">Train/Test Splits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-how-well-we-do">Measuring How Well We Do</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collective-classification">Collective Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-classification">Iterative Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">Your Turn!</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#link-prediction">Link Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-based-methods">Similarity-Based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-limited-whirlwind-tour-of-similarity-measures">A Limited Whirlwind Tour of Similarity Measures</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#for-discussion">For Discussion:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifier-based-methods">Classifier-Based Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#freeform-experimentation">Freeform Experimentation!</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-time">Next time…</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-resources">References and further resources:</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Brennan Klein
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>