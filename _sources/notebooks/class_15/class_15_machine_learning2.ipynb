{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059d53e0",
   "metadata": {},
   "source": [
    "# Class 15: Machine Learning 2 — Building up to Graph Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521f125-5915-4b33-842e-4eb37fbf6cae",
   "metadata": {},
   "source": [
    "## Today's Goals\n",
    "1. Think about how graph neural networks operate at a high level.\n",
    "2. Understand what the building blocks of a graph neural network are.\n",
    "3. Play with a graph neural network's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6298654",
   "metadata": {},
   "source": [
    "## Backpropagation (by request):\n",
    "At a very high level, backpropagation is how we adjust our weights, going back from the output layer (and our loss function) all the way back to the weights from the input layer. \n",
    "\n",
    "It involves computing a bunch of partial derivatives (gradients) and adjusting our weights/biases (the parameters we're learning in our neural network) according to the relationship between the gradients and the loss. \n",
    "\n",
    "### What ingredients do we need to do backpropagation?\n",
    "\n",
    "First, we need a loss function. Our loss function (or cost function) needs to be differentiable with respect to the weights and biases we use in the network. Our loss also has to be expressed as a function of the input and our weights and biases. For example, let's look at a toy example with one hidden layer and a mean squared error (MSE) loss function. \n",
    "\n",
    "![feed-forward NN](https://miro.medium.com/v2/resize:fit:3200/1*ycDUAMaxDYaHx7xl9pqEjg.png)\n",
    "\n",
    "The simplified output $g(\\vec{x})$ of our neural network with input $\\vec{x}$, given weight matrices $W^{(1)}$ and $W^{(2)}$ and generic activation functions $\\sigma^{(1)}$ and $\\sigma^{(2)}$, is\n",
    "\n",
    "$$g(x) = \\sigma^{(2)}(W^{(2)} \\sigma^{(1)}(W^{(1)} \\vec{x}))$$\n",
    "\n",
    "Our loss function, with ground truth $\\vec{z}$, is $\\lvert \\lvert g(\\vec{x}) - \\vec{z} \\rvert \\rvert$. In the generalized sense, we can use a generic loss function $C(g(\\vec{x}), \\vec{z})$. \n",
    "\n",
    "Next, we need partial derivatives and the chain rule!\n",
    "\n",
    "This is how backpropagation adjusts the weights -- we compute the partial derivative of our cost function $C$ with respect to one weight from node $i$ to node $j$ in the $2^{nd}$ matrix of weights:\n",
    "\n",
    "$$\\frac{\\delta C}{\\delta w^{(2)}_{ij}} = \\frac{\\delta C}{\\delta y_j} \\frac{\\delta y_j}{\\delta w^{(2)}_{ij}}$$\n",
    "\n",
    "Here, $y_j$ is the $j^{th}$ output of our network (in the output layer).\n",
    "\n",
    "$$y_j = \\sigma^{(2)}(\\sum_{i}w^{(2)}_{ij} * h_i)$$\n",
    "\n",
    "In other words, we're passing the dot product of row $j$ of $W^{(2)}$ and $\\vec{h}$, our hidden layer's output, through a sigmoid function. Let's call $\\sum_{i}w^{(2)}_{ij} * h_i$ $o_j$, and let's expand our partial derivative expression using the chain rule once more. \n",
    "\n",
    "$$\\frac{\\delta C}{\\delta w^{(2)}_{ij}} = \\frac{\\delta C}{\\delta y_j} \\frac{\\delta y_j}{o_j} \\frac{o_j}{\\delta w^{(2)}_{ij}}$$\n",
    "\n",
    "What are we doing here? We're tracing how our specific weight $w^{(2)}_{ij}$ affects our computed loss for a particular input (or batch of inputs). \n",
    "\n",
    "We know that $\\frac{\\delta y_j}{o_j}$ is the partial derivative of the activation function $\\sigma^{(2)}$. \n",
    "\n",
    "Additionally, we know that $\\frac{o_j}{\\delta w^{(2)}_{ij}}$ is \n",
    "\n",
    "$$\\frac{\\delta}{\\delta w^{(2)}_{ij}}\\sum_{k}w^{(2)}_{kj}h_k$$\n",
    "\n",
    "Only one term in this sum relies on $w^{(2)}_{ij}$ -- that's $w^{(2)}_{ij} h_i$. This means this part of our partial derivative reduces to  \n",
    "\n",
    "$$\\frac{o_j}{\\delta w^{(2)}_{ij}} = h_i$$\n",
    "\n",
    "Now let's look at $\\frac{\\delta y_j}{h_j}$. Let's say we're using a sigmoid activation function; in this case, this part of our partial derivative is \n",
    "\n",
    "$$\\frac{\\delta}{\\delta h_j}\\sigma(h_j) = \\sigma(h_j) (1 - \\sigma(h_j)) = y_j * (1 - y_j)$$\n",
    "\n",
    "If we're using MSE for the loss function $C$ and $\\vec{z}$ is our ground truth answer, \n",
    "\n",
    "$$\\frac{\\delta C}{\\delta y_j} = 2 (z_j - y_j)$$.\n",
    "\n",
    "Therefore, the gradient of our loss with respect to $w^{(2)}_{ij}$ is \n",
    "\n",
    "$$\\frac{\\delta C}{\\delta w^{(2)}_{ij}} = 2 (z_j - y_j) * y_j * (1 - y_j) * h_i$$.\n",
    "\n",
    "### Moving right along/TL;DR (For those who hate math!!)\n",
    "We take partial derivatives with the chain rule to figure out how much our loss function changes with respect to a particular parameter (like a weight or bias) in the neural network. \n",
    "\n",
    "Then we can change that specific weight with this information. We usually have a learning rate $\\eta$ (or an optimizer that governs the learning rate, which is fancier) that tells us how much to change a weight/bias with respect to our computed gradient.\n",
    "\n",
    "$$\\delta w^{(2)}_{ij} = \\eta \\frac{\\delta C}{\\delta w^{(2)}_{ij}}$$.\n",
    "\n",
    "We don't want to update our parameters too much based on any one example, which is why the learning rate tends to be pretty small (much less than 1) and optimizers will lower the learning rate as training goes on and the model gets better at its task.\n",
    "\n",
    "Let's review how backpropagation works by watching [this video](https://www.youtube.com/watch?v=GlcnxUlrtek&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&index=4). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d796c-54ad-4abf-a0fa-f8cc1756c127",
   "metadata": {},
   "source": [
    "## What do GNNs do?\n",
    "Bottom line: GNNs learn weight matrices that transform node attributes or embeddings. They aggregate information about a node's neighborhood in order to make the next round of node embeddings. These embeddings can then be used for interesting downtream tasks.\n",
    "\n",
    "### Applications\n",
    "\n",
    "**INTERACTIVE MOMENT:**\n",
    "Let's think of some applications for GNNs. What would you use a GNN to do?\n",
    "\n",
    "----------------------\n",
    "Examples: protein-protein interaction prediction, fraud detection, or social network recommendations.\n",
    "\n",
    "### Sidebar: graph isomorphism\n",
    "Graph isomorphism occurs when you have two graph of the same size, one with one set $G$ and one with node set $H$. If you can create a one-to-one mapping $f$ from $G$ to $H$ such every node in $G$ has exactly one corresponding node in $H$ that is indicated by $f(G)$, and two nodes in $f(G)$ are connected if and only if they are also connected in $H$, then you can say that $G$ and $H$ are isomorphic. Basically, if you can rename everyone in a graph with a unique name of an individual in another graph and the structure stays the same, you have two isomorphic graphs. This is important because we want graph neural networks to obey isomorphism. If two graphs are isomorphic and their nodes have the same features, their embeddings should be the same. \n",
    "\n",
    "## How do we learn about nodes' neighborhoods?\n",
    "Nodes in networks notably have neighborhoods of variable size, but we want to represent all nodes with vectors of the same size. So an approach like word2vec might not work if we're trying to aggregate information about a node's neighborhood. Recall that we concatenated one-hot vectors representing the $k$ words surrounding a word in a sentence to form a word's context when training word2vec -- but how do we know which $k$ nodes ought to form the \"context\" of a node with far more than $k$ neighbors? \n",
    "\n",
    "### enter...permutation-invariant functions\n",
    "#### What is a permutation-invariant function?\n",
    "Permutation-invariant functions take in multiple inputs (say, a list of inputs), and they produce the same output regardless of the order in which the inputs are given. So if $f(x, y, z) = f(y, z, x)$, and so on for all orderings of $x, y, z$, then $f$ is permutation-invariant.\n",
    "\n",
    "#### Why do we care about permutation-invariance?\n",
    "Permutation-invariance is really useful for incorporating information about a node's neighborhood in a graph. For example, operations like the mean, maximum, sum, and minimum are all permutation-invariant. We can put in as many nodes' attributes as we'd like, and our output will maintain the same dimensionality. It will also be insensitive to how we order the inputs, so we don't have to worry about how to order data that doesn't come with inherent order. \n",
    "\n",
    "### The three core functionalities in most GNNs\n",
    "#### AGGREGATE\n",
    "In order to pass information through a GNN, we first gather up our information about a node's neighborhood -- this might be a set of node embeddings from a previous layer or the nodes' raw feature vectors. Then, we pass this set of vectors through a permutation-invariant function like MEAN. This aggregates our information about the node's neighborhood into a vector of fixed length. \n",
    "\n",
    "#### COMBINE\n",
    "Next, we need to update our node's embedding. We might concatenate our neighborhood vector with our previous node embedding (or feature vector). Some GNNs will include a node in its own neighborhood during aggregation, thereby bypassing the COMBINE step. This gives us the embedding for the node that will be passed to the next layer. \n",
    "\n",
    "#### READOUT\n",
    "Often we need something more than just node embeddings -- we might need information about the whole graph, in which case we'll need to apply a permutation-invariant function to our entire set of node embeddings produced by our last GNN layer, or we might need to pass individual node embeddings through some linear neural network layers to classify nodes, for example. \n",
    "\n",
    "## Tidbits\n",
    "### The Weisfeiler-Lehman Test and Isomorphism\n",
    "The Weisfeiler-Lehman test (W-L test) is a test of graph isomorphism, which we talked about before. It iteratively assigns colors to nodes in a graph, then updates a node's color by hashing the colors of its neighbors. Some specially formulated GNNs are at least as powerful as the W-L test, although many GNNs that aren't specially formulated for this application can't distinguish graphs that the W-L test *can* distinguish between. For more on this topic, check out this [paper](https://arxiv.org/abs/1810.00826) by Xu et al. or a [cool extension](https://cs.stanford.edu/people/jure/pubs/idgnn-aaai21.pdf) on the idea by You et al.\n",
    "\n",
    "### Challenges with GNNs\n",
    "GNNs get computationally intensive pretty fast, particularly with new transformer-based or attention-based models. They also don't do great when passing long-range signals around the network -- while in theory you could have many, many layers that bring in signals from as many hops away as you have layers, in practice, this causes **oversquashing** and **oversmoothing**. **Oversmoothing** is when all node representations start to resemble each other (see [this paper](https://arxiv.org/abs/1909.03211) for more details), and **oversquashing** happens when you try to fit enormous amounts of information into fixed-length vectors (see [this paper](https://arxiv.org/abs/2006.05205) to learn more). That's why GNNs typically don't have very many layers, although each layer can be quite fancy.\n",
    "\n",
    "## GCNs (as an example of a GNN)\n",
    "\n",
    "Material in this section relies heavily on Maxime Labonne's [blog post](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95) in *Towards Data Science* and Thomas Kipf's [blog post](https://tkipf.github.io/graph-convolutional-networks/) on his GitHub website. \n",
    "\n",
    "A [GCN](https://arxiv.org/pdf/1609.02907) (Kipf & Welling, 2017) is a type of graph neural network that aggregates nodes' neighborhoods in a clever way. It uses insights from image processing to perform the AGGREGATE functionality. It scales nicely (as far as GNNs go), learns about graph structure **and** node features, and performs quite well on graphs with node features & labels. \n",
    "\n",
    "### What is a convolution in image processing world?\n",
    "A **convolution matrix**, or **kernel**, is used in image processing to blur, enhance, sharpen, or detect edges in an image. It's a small matrix (relative to the size of the image) that is applied to each pixel in the image **and its neighbors within a certain distance**. \n",
    "\n",
    "The generic equation for a kernel is this, where $\\omega$ is the kernel matrix, $a$ and $b$ indicate the dimensions of the kernel, and $f(x, y)$ is the $(x, y)^{th}$ pixel of the image:\n",
    "\n",
    "$$g(x, y) = \\sum_{i=-a}^{a} \\sum^{b}_{j=-b} \\omega(i, j) f(x-i, y-j)$$\n",
    "\n",
    "Here, $g(x, y)$ is the $(x, y)^{th}$ pixel of the output image.\n",
    "\n",
    "Here's a visual example of a convolution matrix being applied to a single pixel (from [this article](https://www.researchgate.net/publication/332190148_Best_Practice_Guide_-_Deep_Learning)):\n",
    "![](https://www.researchgate.net/profile/Volker-Weinberg/publication/332190148/figure/fig2/AS:743933420249088@1554378957080/Schematic-illustration-of-a-convolutional-operation-The-convolutional-kernel-shifts-over.ppm)\n",
    "\n",
    "### Graph Convolutions\n",
    "You might say, cool, that's neat, but how does that apply to graphs? First of all, graph neighborhoods are not rectangular in shape, and graphs notably have degree *distributions* - not every node has the same number of neighbors (far from it)! \n",
    "\n",
    "Let's tackle what happens in GCNs at the node level first. We'll look at how we create our first embedding for node $i$, $h_{i}^{(1)}$. \n",
    "\n",
    "We know we need to merge our node features with those of our neighbors, so we define a node $i$'s neighborhood here as $i$'s neighbors plus $i$ itself. We'll denote this as $\\tilde{N_i}$.\n",
    "\n",
    "In the simplest case, we could create a weight matrix $W_i$ and multiply each node $j$'s features $x_j$ by $W_i$, then sum them:\n",
    "\n",
    "$$h_{i}^{(1)} = \\sum_{j \\in \\tilde{N_i}} W^{(1)} x_j$$\n",
    "\n",
    "This seems neat, but there's a small problem.\n",
    "\n",
    "**INTERACTIVE MOMENT**: Nodes in graphs notably don't all have the same degree. What's going to happen to the vectors of high-degree nodes as compared to those of low-degree nodes right now? How might we fix this?\n",
    "\n",
    "--------------------\n",
    "\n",
    "Spoiler alert: we're going to divide by $k_i$, the degree of node $i$. This keeps vector magnitudes around the same-ish size. \n",
    "\n",
    "$$h_{i}^{(1)} = \\frac{1}{k_i}\\sum_{j \\in \\tilde{N_i}} W^{(1)} x_j$$\n",
    "\n",
    "However, there's one more improvement we can make. Kipf and Welling noticed that features from high-degree nodes tended to propagate through the network more easily than those from low-degree nodes. They therefore up-weight the lower-degree nodes' contributions in the following way:\n",
    "\n",
    "$$h_{i}^{(1)} = \\sum_{j \\in \\tilde{N_i}} \\frac{1}{\\sqrt{k_i}}\\frac{1}{\\sqrt{k_j}} W^{(1)} x_j$$\n",
    "\n",
    "**INTERACTIVE MOMENT**: Why does this work? \n",
    "\n",
    "#### Matrix Formulation\n",
    "There's also a neat way we can formulate this as a matrix multiplication. Here, $\\hat{A}$ is the adjacency matrix with self-loops added, and $\\hat{D}$ is $\\hat{A}$'s degree matrix (i.e. $A + I$). $H^{(l)}$ is the matrix of node embeddings coming into layer $l$, and $W^{(l)}$ is the weight matrix of layer $l$:\n",
    "\n",
    "$$f(H^{(l)}, A) = \\sigma(\\hat{D}^{-\\frac{1}{2}} \\hat{A} \\hat{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)})$$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a356daa-935d-4358-9f8e-2ea6b7fafbf7",
   "metadata": {},
   "source": [
    "Now we're going to play with a GCN instance. First, let's try training the neural network on the [Cora dataset](https://paperswithcode.com/dataset/cora), which is a citation network with 7 classes of publication. There are 2708 publications and 5429 citation links between them. We're going to train a 2-layer GCN on this dataset and see how well it performs on held-out validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7603505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import CitationFull\n",
    "\n",
    "# loading the dataset\n",
    "dataset = CitationFull('/courses/PHYS7332.202510/shared/data/', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "655b7679-8ba6-487b-a60c-8baf97b6af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 4.2583, Accuracy: 0.0478\n",
      "Epoch: 002, Loss: 4.1405, Accuracy: 0.0276\n",
      "Epoch: 003, Loss: 3.9989, Accuracy: 0.0623\n",
      "Epoch: 004, Loss: 3.7828, Accuracy: 0.1479\n",
      "Epoch: 005, Loss: 3.5689, Accuracy: 0.1563\n",
      "Epoch: 006, Loss: 3.3722, Accuracy: 0.2587\n",
      "Epoch: 007, Loss: 3.1978, Accuracy: 0.3254\n",
      "Epoch: 008, Loss: 3.0163, Accuracy: 0.3567\n",
      "Epoch: 009, Loss: 2.8457, Accuracy: 0.3772\n",
      "Epoch: 010, Loss: 2.6924, Accuracy: 0.4136\n",
      "Epoch: 011, Loss: 2.5250, Accuracy: 0.4436\n",
      "Epoch: 012, Loss: 2.3838, Accuracy: 0.4732\n",
      "Epoch: 013, Loss: 2.2103, Accuracy: 0.4938\n",
      "Epoch: 014, Loss: 2.0728, Accuracy: 0.5251\n",
      "Epoch: 015, Loss: 1.9217, Accuracy: 0.5487\n",
      "Epoch: 016, Loss: 1.7906, Accuracy: 0.5800\n",
      "Epoch: 017, Loss: 1.6499, Accuracy: 0.5901\n",
      "Epoch: 018, Loss: 1.5480, Accuracy: 0.6069\n",
      "Epoch: 019, Loss: 1.4414, Accuracy: 0.6154\n",
      "Epoch: 020, Loss: 1.3672, Accuracy: 0.6295\n",
      "Epoch: 021, Loss: 1.2793, Accuracy: 0.6443\n",
      "Epoch: 022, Loss: 1.2053, Accuracy: 0.6474\n",
      "Epoch: 023, Loss: 1.1450, Accuracy: 0.6564\n",
      "Epoch: 024, Loss: 1.0935, Accuracy: 0.6632\n",
      "Epoch: 025, Loss: 1.0423, Accuracy: 0.6679\n",
      "Epoch: 026, Loss: 0.9919, Accuracy: 0.6699\n",
      "Epoch: 027, Loss: 0.9460, Accuracy: 0.6723\n",
      "Epoch: 028, Loss: 0.9046, Accuracy: 0.6703\n",
      "Epoch: 029, Loss: 0.8650, Accuracy: 0.6783\n",
      "Epoch: 030, Loss: 0.8287, Accuracy: 0.6898\n",
      "Epoch: 031, Loss: 0.7844, Accuracy: 0.6905\n",
      "Epoch: 032, Loss: 0.7530, Accuracy: 0.6881\n",
      "Epoch: 033, Loss: 0.7233, Accuracy: 0.6881\n",
      "Epoch: 034, Loss: 0.6959, Accuracy: 0.6918\n",
      "Epoch: 035, Loss: 0.6601, Accuracy: 0.6952\n",
      "Epoch: 036, Loss: 0.6347, Accuracy: 0.6932\n",
      "Epoch: 037, Loss: 0.6134, Accuracy: 0.6932\n",
      "Epoch: 038, Loss: 0.5921, Accuracy: 0.6969\n",
      "Epoch: 039, Loss: 0.5692, Accuracy: 0.6979\n",
      "Epoch: 040, Loss: 0.5464, Accuracy: 0.7023\n",
      "Epoch: 041, Loss: 0.5226, Accuracy: 0.7009\n",
      "Epoch: 042, Loss: 0.5065, Accuracy: 0.6972\n",
      "Epoch: 043, Loss: 0.4911, Accuracy: 0.6986\n",
      "Epoch: 044, Loss: 0.4743, Accuracy: 0.6982\n",
      "Epoch: 045, Loss: 0.4558, Accuracy: 0.6948\n",
      "Epoch: 046, Loss: 0.4405, Accuracy: 0.6989\n",
      "Epoch: 047, Loss: 0.4239, Accuracy: 0.6975\n",
      "Epoch: 048, Loss: 0.4094, Accuracy: 0.6992\n",
      "Epoch: 049, Loss: 0.3972, Accuracy: 0.6945\n",
      "Epoch: 050, Loss: 0.3856, Accuracy: 0.7012\n",
      "Epoch: 051, Loss: 0.3733, Accuracy: 0.6996\n",
      "Epoch: 052, Loss: 0.3603, Accuracy: 0.6986\n",
      "Epoch: 053, Loss: 0.3502, Accuracy: 0.7006\n",
      "Epoch: 054, Loss: 0.3386, Accuracy: 0.6996\n",
      "Epoch: 055, Loss: 0.3272, Accuracy: 0.6965\n",
      "Epoch: 056, Loss: 0.3189, Accuracy: 0.6925\n",
      "Epoch: 057, Loss: 0.3124, Accuracy: 0.6932\n",
      "Epoch: 058, Loss: 0.3076, Accuracy: 0.6952\n",
      "Epoch: 059, Loss: 0.3016, Accuracy: 0.6922\n",
      "Epoch: 060, Loss: 0.2941, Accuracy: 0.6982\n",
      "Epoch: 061, Loss: 0.2834, Accuracy: 0.6975\n",
      "Epoch: 062, Loss: 0.2696, Accuracy: 0.6942\n",
      "Epoch: 063, Loss: 0.2638, Accuracy: 0.6945\n",
      "Epoch: 064, Loss: 0.2522, Accuracy: 0.6942\n",
      "Epoch: 065, Loss: 0.2401, Accuracy: 0.6938\n",
      "Epoch: 066, Loss: 0.2357, Accuracy: 0.6989\n",
      "Epoch: 067, Loss: 0.2314, Accuracy: 0.6948\n",
      "Epoch: 068, Loss: 0.2201, Accuracy: 0.6959\n",
      "Epoch: 069, Loss: 0.2098, Accuracy: 0.6938\n",
      "Epoch: 070, Loss: 0.2083, Accuracy: 0.6955\n",
      "Epoch: 071, Loss: 0.2039, Accuracy: 0.6959\n",
      "Epoch: 072, Loss: 0.1930, Accuracy: 0.6948\n",
      "Epoch: 073, Loss: 0.1863, Accuracy: 0.6935\n",
      "Epoch: 074, Loss: 0.1852, Accuracy: 0.6922\n",
      "Epoch: 075, Loss: 0.1788, Accuracy: 0.6952\n",
      "Epoch: 076, Loss: 0.1702, Accuracy: 0.6935\n",
      "Epoch: 077, Loss: 0.1673, Accuracy: 0.6935\n",
      "Epoch: 078, Loss: 0.1638, Accuracy: 0.6925\n",
      "Epoch: 079, Loss: 0.1567, Accuracy: 0.6952\n",
      "Epoch: 080, Loss: 0.1511, Accuracy: 0.6938\n",
      "Epoch: 081, Loss: 0.1490, Accuracy: 0.6938\n",
      "Epoch: 082, Loss: 0.1456, Accuracy: 0.6928\n",
      "Epoch: 083, Loss: 0.1395, Accuracy: 0.6935\n",
      "Epoch: 084, Loss: 0.1365, Accuracy: 0.6878\n",
      "Epoch: 085, Loss: 0.1363, Accuracy: 0.6901\n",
      "Epoch: 086, Loss: 0.1385, Accuracy: 0.6874\n",
      "Epoch: 087, Loss: 0.1503, Accuracy: 0.6787\n",
      "Epoch: 088, Loss: 0.1761, Accuracy: 0.6746\n",
      "Epoch: 089, Loss: 0.2020, Accuracy: 0.6834\n",
      "Epoch: 090, Loss: 0.1682, Accuracy: 0.6891\n",
      "Epoch: 091, Loss: 0.1218, Accuracy: 0.6831\n",
      "Epoch: 092, Loss: 0.1506, Accuracy: 0.6884\n",
      "Epoch: 093, Loss: 0.1248, Accuracy: 0.6854\n",
      "Epoch: 094, Loss: 0.1227, Accuracy: 0.6895\n",
      "Epoch: 095, Loss: 0.1310, Accuracy: 0.6932\n",
      "Epoch: 096, Loss: 0.1050, Accuracy: 0.6841\n",
      "Epoch: 097, Loss: 0.1246, Accuracy: 0.6884\n",
      "Epoch: 098, Loss: 0.1033, Accuracy: 0.6871\n",
      "Epoch: 099, Loss: 0.1087, Accuracy: 0.6861\n",
      "Epoch: 100, Loss: 0.1025, Accuracy: 0.6864\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "# checking if the GPU is available; else use the CPU. \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device) # yell if you don't see 'cuda' here!\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # we're making two GCN convolutional layers here!\n",
    "        self.gcn1 = GCNConv(dataset.num_features, 64) \n",
    "        # the first one takes in the raw node features and outputs a vector of length 64.\n",
    "        self.gcn2 = GCNConv(64, 16)\n",
    "        # the second one takes in the output of gcn1 and outputs a vector of length 16.\n",
    "        self.out = Linear(16, dataset.num_classes)  \n",
    "        # then we make a one-hot vector with an entry for each class in the dataset.\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        h1 = self.gcn1(x, edge_index).relu() # we use a ReLU activation function.\n",
    "        h2 = self.gcn2(h1, edge_index).relu() # this indicates how data moves through the network.\n",
    "        z = self.out(h2)\n",
    "        return h2, z\n",
    "\n",
    "splits = RandomNodeSplit(split='train_rest', num_val=0.15, num_test=0.15)(dataset.data)\n",
    "# this lets us make a mask on our dataset \n",
    "# such that we're only training the model on a subset of nodes.\n",
    "# we have a validation set that we look at each epoch to track our accuracy\n",
    "# as well as a test set that we can use to look at our performance at the end of training.\n",
    "\n",
    "model = GCN() #instantiates our GCN\n",
    "model.to(device) # puts it on the GPU\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "# cross-entropy loss tells us how wrong given our final output \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02) \n",
    "# you can mess with the learning rate or choice of optimizer\n",
    "loader = DataLoader(dataset)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train() # keeps track of gradients; this is memory-intensive.\n",
    "    total_loss = 0 # keep track of loss\n",
    "    tot_accuracy = 0 # keep track of \n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad() \n",
    "        # zero the gradient so we aren't accumulating them unnecessarily\n",
    "        h2, z = model(batch.x.to(device), batch.edge_index.to(device)) \n",
    "        # make sure we're putting our data on GPU\n",
    "        loss = criterion(z[splits.train_mask], batch.y.to(device)[splits.train_mask])\n",
    "        # only do backpropagation based on nodes in the train set.\n",
    "        loss.backward()\n",
    "        # this is the backpropagation step.\n",
    "        optimizer.step()\n",
    "        # optimizers control how backpropagation goes. T\n",
    "        # The fancier ones, like Adam, can adjust the learning rate \n",
    "        # dynamically depending on the magnitude of the gradients.\n",
    "        # AdaGrad can change the learning rate for each rate, so it's really fancy.\n",
    "        total_loss += loss.item() # keep track of our total loss (cross-entropy)\n",
    "    model.eval() # put the model in eval mode - don't accumulate gradients.\n",
    "    # this saves memory!\n",
    "    val_h, val_z = model(dataset.x.to(device), dataset.edge_index.to(device)) \n",
    "    # run our dataset through the model\n",
    "    val_z = val_z[splits.val_mask]\n",
    "    # look only at the validation set's vectors\n",
    "    ans = val_z.argmax(dim=1) \n",
    "    # what predictions did we get for the classes?\n",
    "    ys = batch.y.to(device)[splits.val_mask]\n",
    "    tot_accuracy += torch.mean(torch.eq(ans, ys).float()) # how often were we right?\n",
    "    loss = total_loss / len(loader)\n",
    "    accuracy = tot_accuracy / len(loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bd644-eb75-484a-8886-e9cf32ef406a",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "For this Your Turn section, I want you to do one or more of the following:\n",
    "\n",
    "1) Figure out how to make a GCN model with an adjustable number of layers (e.g. `model = GCN(3)` should give me a model that has three GCN layers). Try training the model with several different numbers of layers. Tell me how the performance changes as the number of layers increases/decreases. Optionally, look at the embeddings that the model produces and tell me if their quality changes.\n",
    "\n",
    "2) The [GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) layer takes several different keyword arguments that are its own (e.g. `improved`, `add_self_loops`, `normalize`) or can be inherited from the [MessagePassing](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) class in `torch-geometric`, as GCNConv is a message-passing GNN layer. The `MessagePassing` arguments include a choice of aggregation function and the ability to change the flow of message-passing. Mess with these keyword arguments and keep track of the accuracy and loss as training proceeds for a few settings of, say, aggregation function. Plot your accuracy and/or loss over the course of training for several different settings of the parameter you chose to vary. What do you notice? Why do you think this is the case?\n",
    "\n",
    "3) Look at the different choices of convolutional layers available [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers). Choose a couple different types of convolutional layers and build models with those layers. Which do well on this dataset? Which do worse? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940a498-b118-4346-bd5c-56152a84a674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
